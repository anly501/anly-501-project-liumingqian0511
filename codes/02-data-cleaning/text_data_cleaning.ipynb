{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This page will show you how I cleaned up the text data that I gathered from Twitter API. The cleaning of text data is very different from that of record data. The goal of text data cleanup is to make raw text standardized and uniform in format for later analysis. The text data to be cleaned on this page is gather from twitter api. So each unit in the data is a tweet posted by the user.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the raw json file [here](https://github.com/anly501/anly-501-project-liumingqian0511/tree/main/data/00-raw-data/twitter_data). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk;\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import string \n",
    "import json\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Read in Json File\n",
    "================================================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The first step is to read in the json file that needs to be cleaned and open it with the pd.read_json() function. One good thing about the pd.read_json() function is that every json file opened with it automatically converts to a dataframe which is more operable. I assigned the file to 'health_insurance_df'. I created an 'ID' column that take value from 1 to the length of the data frame for later use. After adding 'ID' column, we can see that the data frame consists of 6 column, we are going to focus on the 'text' column for cleaning purpose.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>lang</th>\n",
       "      <th>author_id</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-09-28 01:16:11+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>1544004181886996480</td>\n",
       "      <td>1574930940291579904</td>\n",
       "      <td>@winter_canada I got 1 so far and have a diffe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-09-28 01:04:48+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>950584668708945920</td>\n",
       "      <td>1574928074285584384</td>\n",
       "      <td>Did you know that you are covered by MediShiel...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-09-28 01:00:21+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>2986463136</td>\n",
       "      <td>1574926957220745216</td>\n",
       "      <td>Everyone deserves to have this kind of health ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-09-28 00:30:00+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>1274040016704962560</td>\n",
       "      <td>1574919318693089280</td>\n",
       "      <td>Your total compensation is more than just your...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-09-28 00:29:57+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>80917722</td>\n",
       "      <td>1574919305912848384</td>\n",
       "      <td>Walmart is teaming up with a fertility startup...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2022-09-28 00:25:35+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>1558902826188906496</td>\n",
       "      <td>1574918206623547392</td>\n",
       "      <td>Best/Top 10 health insurance companies in Indi...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2022-09-28 00:16:02+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>1623244873</td>\n",
       "      <td>1574915800825200640</td>\n",
       "      <td>Enjoy your fall activities without worries kno...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2022-09-27 23:55:09+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>24280970</td>\n",
       "      <td>1574910545526157312</td>\n",
       "      <td>Looking to speak to someone who is aged betwee...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2022-09-27 23:54:17+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>890921916260896768</td>\n",
       "      <td>1574910329884409856</td>\n",
       "      <td>RT @Ampersand48: Subsidized housing and subsid...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2022-09-27 23:49:19+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>636585149</td>\n",
       "      <td>1574909077670469632</td>\n",
       "      <td>@glen_mcgregor You understand there's a bit of...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 created_at lang            author_id                   id  \\\n",
       "0 2022-09-28 01:16:11+00:00   en  1544004181886996480  1574930940291579904   \n",
       "1 2022-09-28 01:04:48+00:00   en   950584668708945920  1574928074285584384   \n",
       "2 2022-09-28 01:00:21+00:00   en           2986463136  1574926957220745216   \n",
       "3 2022-09-28 00:30:00+00:00   en  1274040016704962560  1574919318693089280   \n",
       "4 2022-09-28 00:29:57+00:00   en             80917722  1574919305912848384   \n",
       "5 2022-09-28 00:25:35+00:00   en  1558902826188906496  1574918206623547392   \n",
       "6 2022-09-28 00:16:02+00:00   en           1623244873  1574915800825200640   \n",
       "7 2022-09-27 23:55:09+00:00   en             24280970  1574910545526157312   \n",
       "8 2022-09-27 23:54:17+00:00   en   890921916260896768  1574910329884409856   \n",
       "9 2022-09-27 23:49:19+00:00   en            636585149  1574909077670469632   \n",
       "\n",
       "                                                text  ID  \n",
       "0  @winter_canada I got 1 so far and have a diffe...   1  \n",
       "1  Did you know that you are covered by MediShiel...   2  \n",
       "2  Everyone deserves to have this kind of health ...   3  \n",
       "3  Your total compensation is more than just your...   4  \n",
       "4  Walmart is teaming up with a fertility startup...   5  \n",
       "5  Best/Top 10 health insurance companies in Indi...   6  \n",
       "6  Enjoy your fall activities without worries kno...   7  \n",
       "7  Looking to speak to someone who is aged betwee...   8  \n",
       "8  RT @Ampersand48: Subsidized housing and subsid...   9  \n",
       "9  @glen_mcgregor You understand there's a bit of...  10  "
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "health_insurance_df = pd.read_json('/Users/liumingqian/anly-501-project-liumingqian0511/data/00-raw-data/twitter_data/tweetshealth insurance plan.json')\n",
    "print(type(health_insurance_df))\n",
    "health_insurance_df = health_insurance_df.assign(ID = list(range(1,301)))\n",
    "health_insurance_df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Text\n",
    "================================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The first step in text cleaning is to remove the stop words from the text. Stopwords are the words which does not add much meaning to a sentence. They can safely be ignored without sacrificing the meaning of the sentence. For example, the words like the, I, it etc. Such words are already captured this in corpus named corpus. We first download it to our python environment. I wrote a define function to loop through each tweets and to filter out stopwords and lowercase all the letters. Applying this function to our dataframe, we can see that the 'text' column is now stopwords-free.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterStopwords(df):\n",
    "    for val, tweet in enumerate(df['text']):\n",
    "        new_text=\"\"\n",
    "        for word in nltk.tokenize.word_tokenize(tweet):\n",
    "            if word not in nltk.corpus.stopwords.words('english'):\n",
    "                if word in [\".\",\",\",\"!\",\"?\",\":\",\";\"]:\n",
    "                    #remove the last space\n",
    "                    new_text = new_text[0:-1]+word+\" \"\n",
    "                else: \n",
    "                    #add a space\n",
    "                    new_text+=word.lower()+\" \"\n",
    "        df['text'][val] = new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0y/czdjt9k17_9fgxmg3d286zjm0000gn/T/ipykernel_85033/924867572.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text'][val] = new_text\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>lang</th>\n",
       "      <th>author_id</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-09-28 01:16:11+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>1544004181886996480</td>\n",
       "      <td>1574930940291579904</td>\n",
       "      <td>@ winter_canada i got 1 far different one sche...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-09-28 01:04:48+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>950584668708945920</td>\n",
       "      <td>1574928074285584384</td>\n",
       "      <td>did know covered medishield life singapore cit...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-09-28 01:00:21+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>2986463136</td>\n",
       "      <td>1574926957220745216</td>\n",
       "      <td>everyone deserves kind health insurance. we ’ ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-09-28 00:30:00+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>1274040016704962560</td>\n",
       "      <td>1574919318693089280</td>\n",
       "      <td>your total compensation salary. when receive j...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-09-28 00:29:57+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>80917722</td>\n",
       "      <td>1574919305912848384</td>\n",
       "      <td>walmart teaming fertility startup offer benefi...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2022-09-28 00:25:35+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>1558902826188906496</td>\n",
       "      <td>1574918206623547392</td>\n",
       "      <td>best/top 10 health insurance companies india 2...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2022-09-28 00:16:02+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>1623244873</td>\n",
       "      <td>1574915800825200640</td>\n",
       "      <td>enjoy fall activities without worries knowing ...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2022-09-27 23:55:09+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>24280970</td>\n",
       "      <td>1574910545526157312</td>\n",
       "      <td>looking speak someone aged 25-31 years old sti...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2022-09-27 23:54:17+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>890921916260896768</td>\n",
       "      <td>1574910329884409856</td>\n",
       "      <td>rt @ ampersand48: subsidized housing subsidize...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2022-09-27 23:49:19+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>636585149</td>\n",
       "      <td>1574909077670469632</td>\n",
       "      <td>@ glen_mcgregor you understand 's bit differen...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 created_at lang            author_id                   id  \\\n",
       "0 2022-09-28 01:16:11+00:00   en  1544004181886996480  1574930940291579904   \n",
       "1 2022-09-28 01:04:48+00:00   en   950584668708945920  1574928074285584384   \n",
       "2 2022-09-28 01:00:21+00:00   en           2986463136  1574926957220745216   \n",
       "3 2022-09-28 00:30:00+00:00   en  1274040016704962560  1574919318693089280   \n",
       "4 2022-09-28 00:29:57+00:00   en             80917722  1574919305912848384   \n",
       "5 2022-09-28 00:25:35+00:00   en  1558902826188906496  1574918206623547392   \n",
       "6 2022-09-28 00:16:02+00:00   en           1623244873  1574915800825200640   \n",
       "7 2022-09-27 23:55:09+00:00   en             24280970  1574910545526157312   \n",
       "8 2022-09-27 23:54:17+00:00   en   890921916260896768  1574910329884409856   \n",
       "9 2022-09-27 23:49:19+00:00   en            636585149  1574909077670469632   \n",
       "\n",
       "                                                text  ID  \n",
       "0  @ winter_canada i got 1 far different one sche...   1  \n",
       "1  did know covered medishield life singapore cit...   2  \n",
       "2  everyone deserves kind health insurance. we ’ ...   3  \n",
       "3  your total compensation salary. when receive j...   4  \n",
       "4  walmart teaming fertility startup offer benefi...   5  \n",
       "5  best/top 10 health insurance companies india 2...   6  \n",
       "6  enjoy fall activities without worries knowing ...   7  \n",
       "7  looking speak someone aged 25-31 years old sti...   8  \n",
       "8  rt @ ampersand48: subsidized housing subsidize...   9  \n",
       "9  @ glen_mcgregor you understand 's bit differen...  10  "
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filterStopwords(health_insurance_df)\n",
    "health_insurance_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis\n",
    "================================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The second step of text data cleaning is to perform sentiment analysis for each tweet and output their scores. Sentiment analysis is a technique that detects the underlying sentiment in a piece of text. It is the process of classifying text as either positive, negative, or neutral. Sentiment analysis is very essential to gauge customers or users response. In the following chunks, I wrote a getSentiments() function to rate each tweets' sentiment scores in positivity, negativity, and neutrality. I converted the result from the dictionary to a data frame 'score', and also added a column 'ID' that takes the same value as the 'ID' column in the health_insurance_df. Displaying the first ten rows of the score data frame, we can see that we have four columns of values to rate the corresponting sentiment and one column of 'ID' for later use.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSentiments(df):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    tweet_str = \"\"\n",
    "    tweetscore = []\n",
    "    for tweet in df['text']:\n",
    "        tweet_str = tweet_str + \" \" + tweet\n",
    "        score = sia.polarity_scores(tweet_str)\n",
    "        tweetscore.append(score)\n",
    "    return tweetscore\n",
    "\n",
    "sentiment = getSentiments(health_insurance_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.048</td>\n",
       "      <td>0.952</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.1027</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.028</td>\n",
       "      <td>0.972</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.1027</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.020</td>\n",
       "      <td>0.932</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.4588</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.014</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.7845</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.012</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.9260</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.010</td>\n",
       "      <td>0.840</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.9565</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.008</td>\n",
       "      <td>0.804</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.9834</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.007</td>\n",
       "      <td>0.823</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.9834</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.014</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.9828</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.012</td>\n",
       "      <td>0.819</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.9895</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     neg    neu    pos  compound  ID\n",
       "0  0.048  0.952  0.000   -0.1027   1\n",
       "1  0.028  0.972  0.000   -0.1027   2\n",
       "2  0.020  0.932  0.048    0.4588   3\n",
       "3  0.014  0.900  0.086    0.7845   4\n",
       "4  0.012  0.850  0.138    0.9260   5\n",
       "5  0.010  0.840  0.149    0.9565   6\n",
       "6  0.008  0.804  0.188    0.9834   7\n",
       "7  0.007  0.823  0.170    0.9834   8\n",
       "8  0.014  0.828  0.158    0.9828   9\n",
       "9  0.012  0.819  0.170    0.9895  10"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "score = pd.DataFrame.from_dict(sentiment)\n",
    "score = score.assign(ID = list(range(1,301)))\n",
    "score.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we inner join the score data with the health_insurance_df data so that each tweet in the health_insurance dataframe will have coresponding sentiment scores.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>lang</th>\n",
       "      <th>author_id</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>ID</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-09-28 01:16:11+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>1544004181886996480</td>\n",
       "      <td>1574930940291579904</td>\n",
       "      <td>@ winter_canada i got 1 far different one sche...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.952</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.1027</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-09-28 01:04:48+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>950584668708945920</td>\n",
       "      <td>1574928074285584384</td>\n",
       "      <td>did know covered medishield life singapore cit...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.972</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.1027</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-09-28 01:00:21+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>2986463136</td>\n",
       "      <td>1574926957220745216</td>\n",
       "      <td>everyone deserves kind health insurance. we ’ ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.932</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.4588</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-09-28 00:30:00+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>1274040016704962560</td>\n",
       "      <td>1574919318693089280</td>\n",
       "      <td>your total compensation salary. when receive j...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.7845</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-09-28 00:29:57+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>80917722</td>\n",
       "      <td>1574919305912848384</td>\n",
       "      <td>walmart teaming fertility startup offer benefi...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.9260</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 created_at lang            author_id                   id  \\\n",
       "0 2022-09-28 01:16:11+00:00   en  1544004181886996480  1574930940291579904   \n",
       "1 2022-09-28 01:04:48+00:00   en   950584668708945920  1574928074285584384   \n",
       "2 2022-09-28 01:00:21+00:00   en           2986463136  1574926957220745216   \n",
       "3 2022-09-28 00:30:00+00:00   en  1274040016704962560  1574919318693089280   \n",
       "4 2022-09-28 00:29:57+00:00   en             80917722  1574919305912848384   \n",
       "\n",
       "                                                text  ID    neg    neu    pos  \\\n",
       "0  @ winter_canada i got 1 far different one sche...   1  0.048  0.952  0.000   \n",
       "1  did know covered medishield life singapore cit...   2  0.028  0.972  0.000   \n",
       "2  everyone deserves kind health insurance. we ’ ...   3  0.020  0.932  0.048   \n",
       "3  your total compensation salary. when receive j...   4  0.014  0.900  0.086   \n",
       "4  walmart teaming fertility startup offer benefi...   5  0.012  0.850  0.138   \n",
       "\n",
       "   compound label  \n",
       "0   -0.1027   neu  \n",
       "1   -0.1027   neu  \n",
       "2    0.4588   neu  \n",
       "3    0.7845   neu  \n",
       "4    0.9260   neu  "
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "health_insurance_df = health_insurance_df.merge(score,how='inner')\n",
    "health_insurance_df['label'] =health_insurance_df[['neg','neu','pos']].idxmax(axis=1)\n",
    "health_insurance_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tidy Dataframe\n",
    "================================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**By the previous step, the basic Text cleaning was done. Now we're going to finish up our data frame by renaming columns to more intuitive names, casting the data type, and adding a column to dispplay the label of of the tweet sentiment.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>language</th>\n",
       "      <th>text</th>\n",
       "      <th>ID</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-09-28</td>\n",
       "      <td>en</td>\n",
       "      <td>@ winter_canada i got 1 far different one sche...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.952</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.1027</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-09-28</td>\n",
       "      <td>en</td>\n",
       "      <td>did know covered medishield life singapore cit...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.972</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.1027</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-09-28</td>\n",
       "      <td>en</td>\n",
       "      <td>everyone deserves kind health insurance. we ’ ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.932</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.4588</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-09-28</td>\n",
       "      <td>en</td>\n",
       "      <td>your total compensation salary. when receive j...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.7845</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-09-28</td>\n",
       "      <td>en</td>\n",
       "      <td>walmart teaming fertility startup offer benefi...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.9260</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2022-09-28</td>\n",
       "      <td>en</td>\n",
       "      <td>best/top 10 health insurance companies india 2...</td>\n",
       "      <td>6</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.840</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.9565</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2022-09-28</td>\n",
       "      <td>en</td>\n",
       "      <td>enjoy fall activities without worries knowing ...</td>\n",
       "      <td>7</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.804</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.9834</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2022-09-27</td>\n",
       "      <td>en</td>\n",
       "      <td>looking speak someone aged 25-31 years old sti...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.823</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.9834</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2022-09-27</td>\n",
       "      <td>en</td>\n",
       "      <td>rt @ ampersand48: subsidized housing subsidize...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.9828</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2022-09-27</td>\n",
       "      <td>en</td>\n",
       "      <td>@ glen_mcgregor you understand 's bit differen...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.819</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.9895</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date language                                               text  ID  \\\n",
       "0  2022-09-28       en  @ winter_canada i got 1 far different one sche...   1   \n",
       "1  2022-09-28       en  did know covered medishield life singapore cit...   2   \n",
       "2  2022-09-28       en  everyone deserves kind health insurance. we ’ ...   3   \n",
       "3  2022-09-28       en  your total compensation salary. when receive j...   4   \n",
       "4  2022-09-28       en  walmart teaming fertility startup offer benefi...   5   \n",
       "5  2022-09-28       en  best/top 10 health insurance companies india 2...   6   \n",
       "6  2022-09-28       en  enjoy fall activities without worries knowing ...   7   \n",
       "7  2022-09-27       en  looking speak someone aged 25-31 years old sti...   8   \n",
       "8  2022-09-27       en  rt @ ampersand48: subsidized housing subsidize...   9   \n",
       "9  2022-09-27       en  @ glen_mcgregor you understand 's bit differen...  10   \n",
       "\n",
       "     neg    neu    pos  compound label  \n",
       "0  0.048  0.952  0.000   -0.1027   neu  \n",
       "1  0.028  0.972  0.000   -0.1027   neu  \n",
       "2  0.020  0.932  0.048    0.4588   neu  \n",
       "3  0.014  0.900  0.086    0.7845   neu  \n",
       "4  0.012  0.850  0.138    0.9260   neu  \n",
       "5  0.010  0.840  0.149    0.9565   neu  \n",
       "6  0.008  0.804  0.188    0.9834   neu  \n",
       "7  0.007  0.823  0.170    0.9834   neu  \n",
       "8  0.014  0.828  0.158    0.9828   neu  \n",
       "9  0.012  0.819  0.170    0.9895   neu  "
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "health_insurance_df['created_at'] = health_insurance_df['created_at'].apply(lambda x: x.date)\n",
    "health_insurance_df.rename(columns={'created_at':'date','lang':'language'},inplace = True)\n",
    "health_insurance_df.drop(columns = ['author_id','id'],inplace = True)\n",
    "health_insurance_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing Text Data\n",
    "================================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In programming, a vector is a data structure that is similar to a list or an array. For the purpose of input representation, it is simply a succession of values, with the number of values representing the vector’s “dimensionality.” Text Vectorization is the process of converting text into numerical representation. I extracted the text from each tweet and save them both to a string for wordcloud and to a list for vectorizing. Using the CountVectorizer() function from the sklearn library, we can convert the corpus to a dense matrix. I transformed the matrix to a data frame which each column take a word. This gives us a 300 x 1026 huge data frame.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_str = \"\"\n",
    "corpus = []\n",
    "health_insurance_df['text'].apply(lambda x: corpus.append(x))\n",
    "corpus_str = corpus_str.join(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>1fr33dom</th>\n",
       "      <th>20</th>\n",
       "      <th>200</th>\n",
       "      <th>2000s</th>\n",
       "      <th>2008</th>\n",
       "      <th>2020</th>\n",
       "      <th>2022</th>\n",
       "      <th>...</th>\n",
       "      <th>yet</th>\n",
       "      <th>ymooqbi3xa</th>\n",
       "      <th>yo52ognxt8</th>\n",
       "      <th>you</th>\n",
       "      <th>your</th>\n",
       "      <th>yzprsx9mt3</th>\n",
       "      <th>zackdunn314159</th>\n",
       "      <th>zekegary2</th>\n",
       "      <th>zero</th>\n",
       "      <th>znxpynpvhf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1026 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   000  10  11  1fr33dom  20  200  2000s  2008  2020  2022  ...  yet  \\\n",
       "0    0   0   0         0   0    0      0     0     0     0  ...    0   \n",
       "1    0   0   0         0   0    0      0     0     0     0  ...    0   \n",
       "2    0   0   0         0   0    0      0     0     0     0  ...    1   \n",
       "3    0   0   1         0   0    0      0     0     0     0  ...    0   \n",
       "4    0   0   0         0   0    0      0     0     0     0  ...    0   \n",
       "\n",
       "   ymooqbi3xa  yo52ognxt8  you  your  yzprsx9mt3  zackdunn314159  zekegary2  \\\n",
       "0           0           0    0     0           0               0          0   \n",
       "1           0           0    0     0           0               0          0   \n",
       "2           0           0    0     0           0               0          0   \n",
       "3           0           0    0     1           1               0          0   \n",
       "4           1           0    0     0           0               0          0   \n",
       "\n",
       "   zero  znxpynpvhf  \n",
       "0     0           0  \n",
       "1     0           0  \n",
       "2     0           1  \n",
       "3     0           0  \n",
       "4     0           0  \n",
       "\n",
       "[5 rows x 1026 columns]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer=CountVectorizer()\n",
    "Xs  =  vectorizer.fit_transform(corpus)\n",
    "X=np.array(Xs.todense())\n",
    "col_names=vectorizer.get_feature_names_out()\n",
    "vec = pd.DataFrame(X,columns=col_names)\n",
    "vec.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Keep working the vectorized data frame, I summed up the value for each column and sortted them in descending order. By doing this, we are able to get the word frequency in a more intuitional way.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>plan</td>\n",
       "      <td>309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>insurance</td>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>health</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>co</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rt</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>benefits</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>the</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>help</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>care</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word  Frequency\n",
       "0       plan        309\n",
       "1  insurance        279\n",
       "2     health        270\n",
       "3      https        132\n",
       "4         co        132\n",
       "5         rt         87\n",
       "6   benefits         63\n",
       "7        the         54\n",
       "8       help         51\n",
       "9       care         45"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_words = Xs.sum(axis=0) \n",
    "words_freq = [[word, sum_words[0, idx]] for word, idx in vectorizer.vocabulary_.items()]\n",
    "words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "words_freq_df = pd.DataFrame(words_freq,columns=['word','Frequency'])\n",
    "words_freq_df.head(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ANLY501')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c7b1725a53a00cadd46ef15f4ed641a76ecf8b551f4473e176c4bcc6f018dcb6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
