{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk;\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import string \n",
    "import json\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import json\n",
    "import pandas as pd\n",
    "import os, json\n",
    "import glob\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Read in the Json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path_to_json = '/Users/liumingqian/anly-501-project-liumingqian0511/data/00-raw-data/twitter_data/'\n",
    "def readFiles(path):\n",
    "    json_pattern = os.path.join(path,'*.json')\n",
    "    file_list = glob.glob(json_pattern)\n",
    "    df = pd.read_json(file_list[0]) # an empty list to store the data frames\n",
    "    #print(df.head())\n",
    "    for file in file_list[1:]:\n",
    "        data = pd.read_json(file) # read data frame from json file\n",
    "        #print(data.head())\n",
    "        df = pd.concat([df,data],axis=0, ignore_index=True) # append the data frame to the list\n",
    "\n",
    "    #df = pd.concat(dfs,sort=False) # concatenate all the data frames in the list.\n",
    "    return df\n",
    "df = readFiles('/Users/liumingqian/anly-501-project-liumingqian0511/data/00-raw-data/insurance_co')\n",
    "df.drop_duplicates(subset = 'text', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_json('/Users/liumingqian/anly-501-project-liumingqian0511/data/00-raw-data/insurance_co/Aetna.json')\n",
    "df2 = pd.read_json('/Users/liumingqian/anly-501-project-liumingqian0511/data/00-raw-data/insurance_co/Anthem.json')\n",
    "df3 = pd.read_json('/Users/liumingqian/anly-501-project-liumingqian0511/data/00-raw-data/insurance_co/Cigna.json')\n",
    "df4 = pd.read_json('/Users/liumingqian/anly-501-project-liumingqian0511/data/00-raw-data/insurance_co/Humana.json')\n",
    "df5 = pd.read_json('/Users/liumingqian/anly-501-project-liumingqian0511/data/00-raw-data/insurance_co/UnitedHealth.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Assign Labels Manually and Merge Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['label'] = 'Aetna'\n",
    "df2['label'] = 'Anthem'\n",
    "df3['label'] = 'Cigna'\n",
    "df4['label'] = 'Humana'\n",
    "df5['label'] = 'UnitedHealth'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(269, 7)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([df1,df2,df3,df4,df5],ignore_index=True)\n",
    "df.drop_duplicates(subset='text',inplace = True)\n",
    "df = df[df['lang'] == 'en']\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Apply Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSentiments(df):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    tweet_str = \"\"\n",
    "    tweetscore = []\n",
    "    for tweet in df['text']:\n",
    "        tweet_str = tweet_str + \" \" + tweet\n",
    "        score = sia.polarity_scores(tweet_str)\n",
    "        tweetscore.append(score)\n",
    "    return tweetscore\n",
    "\n",
    "sentiment = getSentiments(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.085</td>\n",
       "      <td>0.878</td>\n",
       "      <td>0.037</td>\n",
       "      <td>-0.7278</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.080</td>\n",
       "      <td>0.895</td>\n",
       "      <td>0.025</td>\n",
       "      <td>-0.8485</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.071</td>\n",
       "      <td>0.907</td>\n",
       "      <td>0.022</td>\n",
       "      <td>-0.8485</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.080</td>\n",
       "      <td>0.901</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.9137</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.067</td>\n",
       "      <td>0.882</td>\n",
       "      <td>0.051</td>\n",
       "      <td>-0.6116</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.052</td>\n",
       "      <td>0.844</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.9533</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.048</td>\n",
       "      <td>0.840</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.9692</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     neg    neu    pos  compound  ID\n",
       "0  0.000  1.000  0.000    0.0000   1\n",
       "1  0.000  1.000  0.000    0.0000   2\n",
       "2  0.000  1.000  0.000    0.0000   3\n",
       "3  0.085  0.878  0.037   -0.7278   4\n",
       "4  0.080  0.895  0.025   -0.8485   5\n",
       "5  0.071  0.907  0.022   -0.8485   6\n",
       "6  0.080  0.901  0.019   -0.9137   7\n",
       "7  0.067  0.882  0.051   -0.6116   8\n",
       "8  0.052  0.844  0.104    0.9533   9\n",
       "9  0.048  0.840  0.112    0.9692  10"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = pd.DataFrame.from_dict(sentiment)\n",
    "score = score.assign(ID = list(range(1,len(score)+1)))\n",
    "score.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Add Sentiment Outcome to Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.assign(ID = list(range(1,len(df)+1)))\n",
    "df = df.merge(score,how='inner')\n",
    "df['sentiment'] =df[['neg','neu','pos']].idxmax(axis=1)\n",
    "df['sent_binary'] = df[['neg','pos']].idxmax(axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e) Filter out Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0y/czdjt9k17_9fgxmg3d286zjm0000gn/T/ipykernel_67085/605275336.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text'][val] = new_text\n"
     ]
    }
   ],
   "source": [
    "def filterStopwords(df):\n",
    "    for val, tweet in enumerate(df['text']):\n",
    "        new_text=\"\"\n",
    "        for word in nltk.tokenize.word_tokenize(tweet):\n",
    "            word = word.lower()\n",
    "            if word not in nltk.corpus.stopwords.words('english') and word not in [\".\",\",\",\"!\",\"?\",\":\",\";\",\")\",\"(\",\"'\",\"$\",'https','co','rt','@','de','la','the']:\n",
    "                    new_text+=word+\" \"\n",
    "            df['text'][val] = new_text\n",
    "\n",
    "filterStopwords(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f) Lemmatize Text Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize(df):\n",
    "    for i,tweet in enumerate(df['text']):\n",
    "        new_text = \"\"\n",
    "        for word in tweet.split(' '):\n",
    "            if wn.synsets(word):\n",
    "                new_text += lemmatizer.lemmatize(word,pos= wn.synsets(word)[0].pos()) + \" \"\n",
    "            else:\n",
    "                new_text += word + \" \"\n",
    "        df['text'][i] = new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0y/czdjt9k17_9fgxmg3d286zjm0000gn/T/ipykernel_67085/4089900633.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text'][i] = new_text\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>edit_history_tweet_ids</th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>lang</th>\n",
       "      <th>author_id</th>\n",
       "      <th>label</th>\n",
       "      <th>ID</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sent_binary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1579944177819070464</td>\n",
       "      <td>[1579944177819070465]</td>\n",
       "      <td>loyolaramblers ğ—§ğ—µğ—² ğ—¥ğ—®ğ—ºğ—¯ğ—¹ğ—²ğ—¿ ğ—¥ğ—¼ğ˜‚ğ—»ğ—±ğ˜ğ—®ğ—¯ğ—¹ğ—² present ...</td>\n",
       "      <td>2022-10-11 21:17:00+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>1015398717392687104</td>\n",
       "      <td>Aetna</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neu</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1579940514522370048</td>\n",
       "      <td>[1579940514522370048]</td>\n",
       "      <td>pom_pnw aetna give directory resident house ad...</td>\n",
       "      <td>2022-10-11 21:02:27+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>1325197299409182720</td>\n",
       "      <td>Aetna</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neu</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1579937294152519680</td>\n",
       "      <td>[1579937294152519680]</td>\n",
       "      <td>peds_ortho aetna â€™ doctor â€™ office take copay ...</td>\n",
       "      <td>2022-10-11 20:49:39+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>1228846428929380352</td>\n",
       "      <td>Aetna</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neu</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1579934089695875072</td>\n",
       "      <td>[1579934089695875072]</td>\n",
       "      <td>aetna twice deny necessary medication b/c what...</td>\n",
       "      <td>2022-10-11 20:36:55+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>1077754392433082368</td>\n",
       "      <td>Aetna</td>\n",
       "      <td>4</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.878</td>\n",
       "      <td>0.037</td>\n",
       "      <td>-0.7278</td>\n",
       "      <td>neu</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1579932311864238080</td>\n",
       "      <td>[1579932311864238080]</td>\n",
       "      <td>peds_ortho aetna pay copay meet oop limit â€™ pa...</td>\n",
       "      <td>2022-10-11 20:29:51+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>1228846428929380352</td>\n",
       "      <td>Aetna</td>\n",
       "      <td>5</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.895</td>\n",
       "      <td>0.025</td>\n",
       "      <td>-0.8485</td>\n",
       "      <td>neu</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id edit_history_tweet_ids  \\\n",
       "0  1579944177819070464  [1579944177819070465]   \n",
       "1  1579940514522370048  [1579940514522370048]   \n",
       "2  1579937294152519680  [1579937294152519680]   \n",
       "3  1579934089695875072  [1579934089695875072]   \n",
       "4  1579932311864238080  [1579932311864238080]   \n",
       "\n",
       "                                                text  \\\n",
       "0  loyolaramblers ğ—§ğ—µğ—² ğ—¥ğ—®ğ—ºğ—¯ğ—¹ğ—²ğ—¿ ğ—¥ğ—¼ğ˜‚ğ—»ğ—±ğ˜ğ—®ğ—¯ğ—¹ğ—² present ...   \n",
       "1  pom_pnw aetna give directory resident house ad...   \n",
       "2  peds_ortho aetna â€™ doctor â€™ office take copay ...   \n",
       "3  aetna twice deny necessary medication b/c what...   \n",
       "4  peds_ortho aetna pay copay meet oop limit â€™ pa...   \n",
       "\n",
       "                 created_at lang            author_id  label  ID    neg  \\\n",
       "0 2022-10-11 21:17:00+00:00   en  1015398717392687104  Aetna   1  0.000   \n",
       "1 2022-10-11 21:02:27+00:00   en  1325197299409182720  Aetna   2  0.000   \n",
       "2 2022-10-11 20:49:39+00:00   en  1228846428929380352  Aetna   3  0.000   \n",
       "3 2022-10-11 20:36:55+00:00   en  1077754392433082368  Aetna   4  0.085   \n",
       "4 2022-10-11 20:29:51+00:00   en  1228846428929380352  Aetna   5  0.080   \n",
       "\n",
       "     neu    pos  compound sentiment sent_binary  \n",
       "0  1.000  0.000    0.0000       neu         neg  \n",
       "1  1.000  0.000    0.0000       neu         neg  \n",
       "2  1.000  0.000    0.0000       neu         neg  \n",
       "3  0.878  0.037   -0.7278       neu         neg  \n",
       "4  0.895  0.025   -0.8485       neu         neg  "
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g) Tidy Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>language</th>\n",
       "      <th>label</th>\n",
       "      <th>ID</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sent_binary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>loyolaramblers ğ—§ğ—µğ—² ğ—¥ğ—®ğ—ºğ—¯ğ—¹ğ—²ğ—¿ ğ—¥ğ—¼ğ˜‚ğ—»ğ—±ğ˜ğ—®ğ—¯ğ—¹ğ—² present ...</td>\n",
       "      <td>2022-10-11</td>\n",
       "      <td>en</td>\n",
       "      <td>Aetna</td>\n",
       "      <td>1</td>\n",
       "      <td>neu</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pom_pnw aetna give directory resident house ad...</td>\n",
       "      <td>2022-10-11</td>\n",
       "      <td>en</td>\n",
       "      <td>Aetna</td>\n",
       "      <td>2</td>\n",
       "      <td>neu</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>peds_ortho aetna â€™ doctor â€™ office take copay ...</td>\n",
       "      <td>2022-10-11</td>\n",
       "      <td>en</td>\n",
       "      <td>Aetna</td>\n",
       "      <td>3</td>\n",
       "      <td>neu</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aetna twice deny necessary medication b/c what...</td>\n",
       "      <td>2022-10-11</td>\n",
       "      <td>en</td>\n",
       "      <td>Aetna</td>\n",
       "      <td>4</td>\n",
       "      <td>neu</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>peds_ortho aetna pay copay meet oop limit â€™ pa...</td>\n",
       "      <td>2022-10-11</td>\n",
       "      <td>en</td>\n",
       "      <td>Aetna</td>\n",
       "      <td>5</td>\n",
       "      <td>neu</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text        date language  \\\n",
       "0  loyolaramblers ğ—§ğ—µğ—² ğ—¥ğ—®ğ—ºğ—¯ğ—¹ğ—²ğ—¿ ğ—¥ğ—¼ğ˜‚ğ—»ğ—±ğ˜ğ—®ğ—¯ğ—¹ğ—² present ...  2022-10-11       en   \n",
       "1  pom_pnw aetna give directory resident house ad...  2022-10-11       en   \n",
       "2  peds_ortho aetna â€™ doctor â€™ office take copay ...  2022-10-11       en   \n",
       "3  aetna twice deny necessary medication b/c what...  2022-10-11       en   \n",
       "4  peds_ortho aetna pay copay meet oop limit â€™ pa...  2022-10-11       en   \n",
       "\n",
       "   label  ID sentiment sent_binary  \n",
       "0  Aetna   1       neu         neg  \n",
       "1  Aetna   2       neu         neg  \n",
       "2  Aetna   3       neu         neg  \n",
       "3  Aetna   4       neu         neg  \n",
       "4  Aetna   5       neu         neg  "
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['created_at'] = df['created_at'].apply(lambda x: x.date)\n",
    "df.rename(columns={'created_at':'date','lang':'language'},inplace = True)\n",
    "df.drop(columns = ['author_id','id','edit_history_tweet_ids','neg','pos','neu','compound'],inplace = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### h) Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_str = \"\"\n",
    "corpus = []\n",
    "df['text'].apply(lambda x: corpus.append(x))\n",
    "corpus_str = corpus_str.join(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>08</th>\n",
       "      <th>0bl5dzrcc2</th>\n",
       "      <th>0cfllxypoq</th>\n",
       "      <th>0skzqs8f8h</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>...</th>\n",
       "      <th>yoy</th>\n",
       "      <th>yuvo</th>\n",
       "      <th>z4wc2sk8ji</th>\n",
       "      <th>zero</th>\n",
       "      <th>zoryalondonsk</th>\n",
       "      <th>zwsyhywsgy</th>\n",
       "      <th>ğ—¥ğ—®ğ—ºğ—¯ğ—¹ğ—²ğ—¿</th>\n",
       "      <th>ğ—¥ğ—¼ğ˜‚ğ—»ğ—±ğ˜ğ—®ğ—¯ğ—¹ğ—²</th>\n",
       "      <th>ğ—§ğ—µğ—²</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Aetna</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Aetna</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Aetna</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Aetna</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Aetna</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 2162 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  000  01  02  08  0bl5dzrcc2  0cfllxypoq  0skzqs8f8h  10  100  ...  yoy  \\\n",
       "0   0    0   0   0   0           0           0           0   0    0  ...    0   \n",
       "1   0    0   0   0   0           0           0           0   0    0  ...    0   \n",
       "2   0    0   0   0   0           0           0           0   0    0  ...    0   \n",
       "3   0    0   0   0   0           0           0           0   0    0  ...    0   \n",
       "4   0    0   0   0   0           0           0           0   0    0  ...    0   \n",
       "\n",
       "   yuvo  z4wc2sk8ji  zero  zoryalondonsk  zwsyhywsgy  ğ—¥ğ—®ğ—ºğ—¯ğ—¹ğ—²ğ—¿  ğ—¥ğ—¼ğ˜‚ğ—»ğ—±ğ˜ğ—®ğ—¯ğ—¹ğ—²  \\\n",
       "0     0           0     0              0           0        1           1   \n",
       "1     0           0     0              0           0        0           0   \n",
       "2     0           0     0              0           0        0           0   \n",
       "3     0           0     0              0           0        0           0   \n",
       "4     0           0     0              0           0        0           0   \n",
       "\n",
       "   ğ—§ğ—µğ—²  label  \n",
       "0    1  Aetna  \n",
       "1    0  Aetna  \n",
       "2    0  Aetna  \n",
       "3    0  Aetna  \n",
       "4    0  Aetna  \n",
       "\n",
       "[5 rows x 2162 columns]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer=CountVectorizer()\n",
    "Xs  =  vectorizer.fit_transform(corpus)\n",
    "X = np.array(Xs.todense())\n",
    "col_names=vectorizer.get_feature_names_out()\n",
    "vec = pd.DataFrame(X,columns=col_names)\n",
    "vec['label'] = df['label']\n",
    "vec.to_csv('/Users/liumingqian/anly-501-project-liumingqian0511/data/01-modified-data/vec.csv')\n",
    "vec.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>co</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cigna</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aetna</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>anthem</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>unitedhealth</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>health</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>get</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>group</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>amp</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>plan</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           word  Frequency\n",
       "0            co        146\n",
       "1         cigna         81\n",
       "2         aetna         77\n",
       "3        anthem         53\n",
       "4  unitedhealth         49\n",
       "5        health         40\n",
       "6           get         29\n",
       "7         group         29\n",
       "8           amp         28\n",
       "9          plan         27"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_words = Xs.sum(axis=0) \n",
    "words_freq = [[word, sum_words[0, idx]] for word, idx in vectorizer.vocabulary_.items()]\n",
    "words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "words_freq_df = pd.DataFrame(words_freq,columns=['word','Frequency'])\n",
    "words_freq_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_freq_df.to_csv('/Users/liumingqian/anly-501-project-liumingqian0511/data/01-modified-data/word_frequency_text.csv')\n",
    "df.to_csv('/Users/liumingqian/anly-501-project-liumingqian0511/data/01-modified-data/text.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ANLY501')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c7b1725a53a00cadd46ef15f4ed641a76ecf8b551f4473e176c4bcc6f018dcb6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
