<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.251">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2022-10-24">

<title>Decision Tree on Record Data</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="decision_tree_files/libs/clipboard/clipboard.min.js"></script>
<script src="decision_tree_files/libs/quarto-html/quarto.js"></script>
<script src="decision_tree_files/libs/quarto-html/popper.min.js"></script>
<script src="decision_tree_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="decision_tree_files/libs/quarto-html/anchor.min.js"></script>
<link href="decision_tree_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="decision_tree_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="decision_tree_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="decision_tree_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="decision_tree_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script src="decision_tree_files/libs/quarto-diagram/mermaid.min.js"></script>
<script src="decision_tree_files/libs/quarto-diagram/mermaid-init.js"></script>
<link href="decision_tree_files/libs/quarto-diagram/mermaid.css" rel="stylesheet">

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Contents</h2>
   
  <ul>
  <li><a href="#methods" id="toc-methods" class="nav-link active" data-scroll-target="#methods">Methods</a></li>
  <li><a href="#class-distribution" id="toc-class-distribution" class="nav-link" data-scroll-target="#class-distribution">Class distribution</a></li>
  <li><a href="#baseline-model" id="toc-baseline-model" class="nav-link" data-scroll-target="#baseline-model">Baseline Model</a></li>
  <li><a href="#feature-selection" id="toc-feature-selection" class="nav-link" data-scroll-target="#feature-selection">Feature Selection</a></li>
  <li><a href="#hyperparameter-tuning" id="toc-hyperparameter-tuning" class="nav-link" data-scroll-target="#hyperparameter-tuning">Hyperparameter Tuning</a></li>
  <li><a href="#final-result-and-conclusion" id="toc-final-result-and-conclusion" class="nav-link" data-scroll-target="#final-result-and-conclusion">Final Result and Conclusion</a></li>
  </ul>
</nav>
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Decision Tree on Record Data</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
</div>



<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">October 24, 2022</p>
    </div>
  </div>
    
  </div>
  

</header>

<section id="methods" class="level3">
<h3 class="anchored" data-anchor-id="methods">Methods</h3>
<p>A tree has many analogies in real life, and it turns out that it has influenced a wide area of machine learning, covering both classification and regression. In decision analysis, a decision tree can be used to visually and explicitly represent decisions and decision making. As the name goes, it uses a tree-like model of decisions. Before jumping into a simple example, let’s first take a look at some important terminologies in Decision Tree algorithm.</p>
<table class="table">
<thead>
<tr class="header">
<th>Terminology</th>
<th>Meaning</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Root Node</td>
<td>Represents entire population</td>
<td></td>
</tr>
<tr class="even">
<td>Spliting</td>
<td>Process of dividing sample</td>
<td></td>
</tr>
<tr class="odd">
<td>Decision Node</td>
<td>Node splits into further sub (children) nodes</td>
<td></td>
</tr>
<tr class="even">
<td>Leaf Node</td>
<td>Last stage of node (output label)</td>
<td></td>
</tr>
<tr class="odd">
<td>Pruning</td>
<td>The opposite process of splitting</td>
<td></td>
</tr>
</tbody>
</table>
<p>Cheat-Sheet: DT terminologies:</p>
<p><strong>How can an algorithm be represented as a tree</strong></p>
<p>For this let’s consider a very basic example that predicting whether a student will get an A in ANLY501. Below model uses 3 features/attributes/columns, namely desire, attendence and time spent weekly(20hrs). A decision tree is drawn upside down with its root at the top. In this image, the text in black represents a condition/internal <strong>node</strong>, based on which the tree splits into <strong>branches/ edges</strong>. The end of the branch that doesn’t split anymore is the decision/<strong>leaf</strong>, in this case, whether the student get an A or not is represented as text respectively.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<p>
</p><pre class="mermaid" data-tooltip-selector="#mermaid-tooltip-1">flowchart TD
    A[Spend &gt;20hrs/week] -- Yes --&gt; C[A!]
    A[Spend &gt;20hrs/week] -- No --&gt; D[Desire of getting A?]
    D -- Yes --&gt; E[Attendence to lecture?]
    D -- No --&gt; F[No A!]
    E -- Yes --&gt; G[A!]
    E -- No --&gt; H[No A!]

</pre>
<div id="mermaid-tooltip-1" class="mermaidTooltip">

</div>
<p></p>
</div>
</div>
</div>
<p><strong>What is actually going on in the background?</strong></p>
<p>A real dataset will have a lot more features and the above example will just be a branch in a much bigger tree, but we can’t ignore the simplicity of this algorithm. The feature importance is clear and relations can be viewed easily. This methodology is more commonly known as learning decision tree from data and above tree is called Classification tree as the target is to classify student as getting an A or not.</p>
<p>Decision tree learning or growing a tree involves deciding on which features to choose and what conditions to use for splitting, along with knowing when to stop. As mentioned above, a decision tree is composed of nodes, and those nodes are chosen looking for the optimum split of the features. For that purpose, different criteria exist. In the decision tree Python implementation of the scikit-learn library, this is made by the parameter ’criterion‘. This parameter is the function used to measure the quality of a split and it allows users to choose between <strong>’gini‘</strong> or <strong>’entropy‘</strong>.</p>
<p><strong>-Gini Index</strong></p>
<p>The gini impurity is calculated using the following formula:</p>
<p><span class="math display">\[ GiniIndex = 1 - \sum_{j}p^2_{j}
\]</span><br>
Where <span class="math inline">\(p_{j}\)</span> is the probability of class <span class="math inline">\(j\)</span>.</p>
<p>The gini impurity measures the frequency at which any element of the dataset will be mislabelled when it is randomly labeled.</p>
<p><strong>Entropy</strong></p>
<p>The entropy is calculated using the following formula: <span class="math display">\[Entropy = -\sum_{j}p_{j}*log_{2}*p_{j}
\]</span><br>
Where, as before, <span class="math inline">\(p_{j}\)</span> is the probability of class <span class="math inline">\(j\)</span>.</p>
<p>Entropy is a measure of information that indicates the disorder of the features with the target. Similar to the Gini Index, the optimum split is chosen by the feature with less entropy. It gets its maximum value when the probability of the two classes is the same and a node is pure when the entropy has its minimum value, which is 0.</p>
</section>
<section id="class-distribution" class="level3">
<h3 class="anchored" data-anchor-id="class-distribution">Class distribution</h3>
<p>Now let us come back to our reord dataset and implement the Decision Tree algorithm on it to classify the outcome variable – Health Index. Since the outcome variable health index is a continuous variable which takes value from 0-100 and also since we want to do a classficication instead of a regression here, I binned the column to multiple classes and assign them with a label. The following table shows the range, count and ratio of each label：</p>
<table class="table">
<thead>
<tr class="header">
<th>Health Index</th>
<th>Label</th>
<th>count</th>
<th>Probability</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Below 60</td>
<td>0</td>
<td>974</td>
<td>0.381</td>
</tr>
<tr class="even">
<td>60-75</td>
<td>1</td>
<td>903</td>
<td>0.354</td>
</tr>
<tr class="odd">
<td>&gt;75</td>
<td>2</td>
<td>677</td>
<td>0.265</td>
</tr>
</tbody>
</table>
<p>According to both the printed table and barplot, we can say that the distribution of three labels is pretty balanced.</p>
<div class="cell" data-execution_count="1">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix, ConfusionMatrixDisplay</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> tree</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> precision_score</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> recall_score</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_selection <span class="im">import</span> VarianceThreshold</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="2">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">'/Users/liumingqian/anly-501-project-liumingqian0511/data/01-modified-data/rand.csv'</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>conditions <span class="op">=</span> [</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    (df[<span class="st">'ghindxx'</span>] <span class="op">&lt;</span> <span class="dv">60</span>),</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    (df[<span class="st">'ghindxx'</span>] <span class="op">&gt;=</span> <span class="dv">60</span>) <span class="op">&amp;</span> (df[<span class="st">'ghindxx'</span>]<span class="op">&lt;</span><span class="dv">75</span>),</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    (df[<span class="st">'ghindxx'</span>] <span class="op">&gt;=</span> <span class="dv">75</span>),</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>values <span class="op">=</span> [<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>]</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'health_index'</span>] <span class="op">=</span> np.select(conditions, values)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>distribution <span class="op">=</span> df[<span class="st">'health_index'</span>].value_counts().reset_index().rename(columns<span class="op">=</span>{<span class="st">'index'</span>: <span class="st">'health_index'</span>,<span class="st">'health_index'</span>:<span class="st">'count'</span>})</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>distribution[<span class="st">'proportion'</span>] <span class="op">=</span> distribution[<span class="st">'count'</span>]<span class="op">/</span><span class="bu">len</span>(df)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'-----------------------------------'</span>)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(distribution)</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'-----------------------------------'</span>)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> sns.barplot(x <span class="op">=</span> <span class="st">'health_index'</span>, y <span class="op">=</span> <span class="st">'count'</span>, data <span class="op">=</span> distribution)</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Barplot of Health Index Range'</span>)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>-----------------------------------
   health_index  count  proportion
0             1    974    0.381363
1             2    903    0.353563
2             0    677    0.265074
-----------------------------------</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="decision_tree_files/figure-html/cell-3-output-2.png" width="602" height="449"></p>
</div>
</div>
</section>
<section id="baseline-model" class="level3">
<h3 class="anchored" data-anchor-id="baseline-model">Baseline Model</h3>
<div class="cell" data-execution_count="3">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> random_classifier(y_data):</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    random.seed(<span class="dv">909391</span>)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    ypred<span class="op">=</span>[]</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    max_label<span class="op">=</span>np.<span class="bu">max</span>(y_data)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>,<span class="bu">len</span>(y_data)):</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>        ypred.append(np.random.choice(np.arange(max_label<span class="op">+</span><span class="dv">1</span>), p <span class="op">=</span> distribution[<span class="st">'proportion'</span>]))</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> ypred</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="4">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> confusion_plot(y_data,y_pred):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'----------------------Classification Report---------------------------'</span>)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'ACCURACY:'</span>, accuracy_score(y_data, y_pred))</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'RECALL (Y = Free):'</span>, recall_score(y_data, y_pred,labels<span class="op">=</span>[<span class="dv">0</span>], average<span class="op">=</span><span class="st">'weighted'</span>))</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'RECALL (Y = Coinsurance):'</span>, recall_score(y_data, y_pred,labels<span class="op">=</span>[<span class="dv">1</span>], average<span class="op">=</span><span class="st">'weighted'</span>))</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'RECALL (Y = Deductible):'</span>, recall_score(y_data, y_pred,labels<span class="op">=</span>[<span class="dv">2</span>], average<span class="op">=</span><span class="st">'weighted'</span>))</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'PRECISION (Y = Free):'</span>, precision_score(y_data,y_pred,labels<span class="op">=</span>[<span class="dv">0</span>],average<span class="op">=</span><span class="st">'weighted'</span>))</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'PRECISION (Y = Coinsurance):'</span>, precision_score(y_data,y_pred,labels<span class="op">=</span>[<span class="dv">1</span>], average<span class="op">=</span><span class="st">'weighted'</span>))</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'PRECISION (Y = Deductible):'</span>, precision_score(y_data,y_pred,labels<span class="op">=</span>[<span class="dv">2</span>], average<span class="op">=</span><span class="st">'weighted'</span>))</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'--------------------------Confusion Matrix-----------------------------'</span>)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(confusion_matrix(y_data,y_pred))</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>    ConfusionMatrixDisplay.from_predictions(y_data, y_pred)</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="5">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>true_label <span class="op">=</span> df[<span class="st">'health_index'</span>]</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>predicted_label <span class="op">=</span> random_classifier(true_label)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>confusion_plot(true_label,predicted_label)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>----------------------Classification Report---------------------------
ACCURACY: 0.32811276429130776
RECALL (Y = Free): 0.39438700147710487
RECALL (Y = Coinsurance): 0.3531827515400411
RECALL (Y = Deductible): 0.2513842746400886
PRECISION (Y = Free): 0.2691532258064516
PRECISION (Y = Coinsurance): 0.39090909090909093
PRECISION (Y = Deductible): 0.33284457478005863
--------------------------Confusion Matrix-----------------------------
[[267 222 188]
 [363 344 267]
 [362 314 227]]</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="decision_tree_files/figure-html/cell-6-output-2.png" width="504" height="429"></p>
</div>
</div>
</section>
<section id="feature-selection" class="level3">
<h3 class="anchored" data-anchor-id="feature-selection">Feature Selection</h3>
<div class="cell" data-execution_count="6">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>conditions <span class="op">=</span> [</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    (df[<span class="st">'plan_type'</span>] <span class="op">==</span> <span class="st">'Free'</span>),</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    (df[<span class="st">'plan_type'</span>] <span class="op">==</span> <span class="st">'Coinsurance'</span>),</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>    (df[<span class="st">'plan_type'</span>] <span class="op">==</span> <span class="st">'Deductible'</span>),</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>    (df[<span class="st">'plan_type'</span>] <span class="op">==</span> <span class="st">'Catastrophic'</span>)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>values <span class="op">=</span> [<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>]</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'plan'</span>] <span class="op">=</span> np.select(conditions, values)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="7">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> df[<span class="st">'health_index'</span>]</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df.drop(columns<span class="op">=</span>[<span class="st">'person'</span>,<span class="st">'plan_type'</span>,<span class="st">'catastrophic'</span>,<span class="st">'free'</span>,<span class="st">'health_index'</span>,<span class="st">'ghindxx'</span>,<span class="st">'face_to_face_visit'</span>,<span class="st">'outpatient_expenses'</span>,<span class="st">'totadm'</span>,<span class="st">'total_expenses'</span>,<span class="st">'inpdol_inf'</span>])</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>ux <span class="op">=</span> np.mean(X,axis <span class="op">=</span> <span class="dv">0</span>) <span class="co"># NORMALIZE X</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>sx <span class="op">=</span> np.std(X,axis <span class="op">=</span> <span class="dv">0</span>)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    X.iloc[:,i] <span class="op">=</span> (X.iloc[:,i] <span class="op">-</span> ux[i])<span class="op">/</span>sx[i]</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="co">#X['plan'] = df['plan']</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>x_train, x_test, y_train, y_test <span class="op">=</span> train_test_split(X, Y, test_size<span class="op">=</span><span class="fl">0.3</span>, random_state<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"TRAINING SHAPES:"</span>,x_train.shape,y_train.shape)</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"TEST SHAPES:"</span>,x_test.shape,y_test.shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>TRAINING SHAPES: (1787, 11) (1787,)
TEST SHAPES: (767, 11) (767,)</code></pre>
</div>
</div>
</section>
<section id="hyperparameter-tuning" class="level2">
<h2 class="anchored" data-anchor-id="hyperparameter-tuning">Hyperparameter Tuning</h2>
<p>Hyperparameter tuning consists of finding a set of optimal hyperparameter values for a learning algorithm while applying this optimized algorithm to any data set. That combination of hyperparameters maximizes the model’s performance, minimizing a predefined loss function to produce better results with fewer errors. In this section, I am going to tune max_depth, min_sample_split and criterion parameter.</p>
<p><strong>Max_depth Tuning:</strong></p>
<p>The first hyperparameter to tune in the Decision Tree is max_depth. It indicates how deep the decision can be. The deeper the tree, the more splits it has and meanwhile captures more information about the data. However, in general a decision tree overfits for large depth values. When the tree perfectly predicts all the training data, it failes to generalize the findings on new data.</p>
<p><strong>Min_sample_split Tuning</strong></p>
<p>The hyperparameter min_sample_split is used to set the minimum number of samples required to split an internal node. This can vary between two extremes, i.e., considering only one sample at each node vs.&nbsp;considering all of the samples at each node - for a given attribute.</p>
<p><strong>Note</strong>: max_depth and min_samples_split are also both related to the computational cost involved with growing the tree. Large values for these parameters can create complex, dense, and long trees. For large datasets, it may become extremely time-consuming to use default values.</p>
<p><strong>Criterion</strong></p>
<p>Supported Criterion are ‘Gini’ and ‘Entropy’.</p>
<div class="cell" data-execution_count="8">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tunningResult(y_train, y_test, yp_train, yp_test,test_result, train_result):</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    accuracy_train <span class="op">=</span> accuracy_score(y_train, yp_train)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    recall_0_train <span class="op">=</span> recall_score(y_train, yp_train,labels<span class="op">=</span>[<span class="dv">0</span>], average<span class="op">=</span><span class="st">'weighted'</span>)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    recall_1_train <span class="op">=</span> recall_score(y_train, yp_train,labels<span class="op">=</span>[<span class="dv">1</span>], average<span class="op">=</span><span class="st">'weighted'</span>)</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    recall_2_train <span class="op">=</span> recall_score(y_train, yp_train,labels<span class="op">=</span>[<span class="dv">2</span>], average<span class="op">=</span><span class="st">'weighted'</span>)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">#recall_3_train = recall_score(y_train, yp_train,labels=[3], average='weighted')</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>    accuracy_test <span class="op">=</span> accuracy_score(y_test, yp_test)</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>    recall_0_test <span class="op">=</span> recall_score(y_test, yp_test,labels<span class="op">=</span>[<span class="dv">0</span>], average<span class="op">=</span><span class="st">'weighted'</span>)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>    recall_1_test <span class="op">=</span> recall_score(y_test, yp_test,labels<span class="op">=</span>[<span class="dv">1</span>], average<span class="op">=</span><span class="st">'weighted'</span>)</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>    recall_2_test <span class="op">=</span> recall_score(y_test, yp_test,labels<span class="op">=</span>[<span class="dv">2</span>], average<span class="op">=</span><span class="st">'weighted'</span>)</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>    <span class="co">#recall_3_test = recall_score(y_test, yp_test,labels=[3], average='weighted')</span></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> [accuracy_train, recall_0_train,recall_1_train,recall_2_train],[accuracy_test,recall_0_test,recall_1_test,recall_2_test]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="9">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>test_results<span class="op">=</span>[]</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>train_results<span class="op">=</span>[]</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> depth <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,<span class="dv">16</span>):</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> tree.DecisionTreeClassifier(max_depth<span class="op">=</span>depth)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> model.fit(x_train,y_train)</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>    yp_train<span class="op">=</span>model.predict(x_train)</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>    yp_test<span class="op">=</span>model.predict(x_test)</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>    a,b <span class="op">=</span> tunningResult(y_train, y_test, yp_train, yp_test,test_results, train_results)</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># print(y_pred.shape)</span></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>    test_results.append([depth]<span class="op">+</span>b)</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>    train_results.append([depth]<span class="op">+</span>a)</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>test_results <span class="op">=</span> pd.DataFrame(test_results)</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>train_results <span class="op">=</span> pd.DataFrame(train_results)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="10">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> resultPlot(train_results,test_results,param):</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    fig,((ax1,ax2),(ax3,ax4)) <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>    fig.set_size_inches(<span class="dv">20</span>,<span class="dv">10</span>)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    ax1.plot(train_results[<span class="dv">0</span>],train_results[<span class="dv">1</span>],<span class="st">'-o'</span>)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    ax1.plot(test_results[<span class="dv">0</span>],test_results[<span class="dv">1</span>],<span class="st">'-o'</span>,color <span class="op">=</span> <span class="st">'darkred'</span>)</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    ax1.set_xlabel(<span class="st">"Number of layers in decision tree"</span> <span class="op">+</span> param)</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>    ax1.set_ylabel(<span class="st">"ACCURACY: Training and Test"</span>)</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>    ax2.plot(train_results[<span class="dv">0</span>],train_results[<span class="dv">2</span>],<span class="st">'-o'</span>)</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>    ax2.plot(test_results[<span class="dv">0</span>],test_results[<span class="dv">2</span>],<span class="st">'-o'</span>,color <span class="op">=</span> <span class="st">'darkred'</span>)</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>    ax2.set_xlabel(<span class="st">"Number of layers in decision tree"</span> <span class="op">+</span> param)</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>    ax2.set_ylabel(<span class="st">"RECALL(Y=Free): Training and Test"</span>)</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>    ax3.plot(train_results[<span class="dv">0</span>],train_results[<span class="dv">3</span>],<span class="st">'-o'</span>)</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>    ax3.plot(test_results[<span class="dv">0</span>],test_results[<span class="dv">3</span>],<span class="st">'-o'</span>,color <span class="op">=</span> <span class="st">'darkred'</span>)</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>    ax3.set_xlabel(<span class="st">"Number of layers in decision tree"</span> <span class="op">+</span> param)</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>    ax3.set_ylabel(<span class="st">"RECALL(Y=Coinsurance): Training and Test"</span>)</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>    ax4.plot(train_results[<span class="dv">0</span>],train_results[<span class="dv">4</span>],<span class="st">'-o'</span>)</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>    ax4.plot(test_results[<span class="dv">0</span>],test_results[<span class="dv">4</span>],<span class="st">'-o'</span>,color <span class="op">=</span> <span class="st">'darkred'</span>)</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>    ax4.set_xlabel(<span class="st">"Number of layers in decision tree"</span> <span class="op">+</span> param)</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>    ax4.set_ylabel(<span class="st">"RECALL(Y=Deductible): Training and Test"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="11">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>resultPlot(train_results,test_results, <span class="st">' max_depth'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="decision_tree_files/figure-html/cell-12-output-1.png" width="1556" height="799"></p>
</div>
</div>
<ul>
<li>Min_sample_split Tuning</li>
</ul>
<div class="cell" data-execution_count="12">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>test_results<span class="op">=</span>[]</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>train_results<span class="op">=</span>[]</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> num_sample <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>,<span class="dv">100</span>):</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> tree.DecisionTreeClassifier(min_samples_split<span class="op">=</span>num_sample )</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> model.fit(x_train,y_train)</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>    yp_train<span class="op">=</span>model.predict(x_train)</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>    yp_test<span class="op">=</span>model.predict(x_test)</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>    a,b <span class="op">=</span> tunningResult(y_train, y_test, yp_train, yp_test,test_results, train_results)</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># print(y_pred.shape)</span></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>    test_results.append([num_sample]<span class="op">+</span>b)</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>    train_results.append([num_sample]<span class="op">+</span>a)</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>test_results <span class="op">=</span> pd.DataFrame(test_results)</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>train_results <span class="op">=</span> pd.DataFrame(train_results)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="13">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>resultPlot(train_results,test_results, <span class="st">' min_sample_split'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="decision_tree_files/figure-html/cell-14-output-1.png" width="1556" height="799"></p>
</div>
</div>
<div class="cell" data-execution_count="14">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>test_results<span class="op">=</span>[]</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>train_results<span class="op">=</span>[]</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> crit <span class="kw">in</span> [<span class="st">'gini'</span>,<span class="st">'entropy'</span>]:</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> tree.DecisionTreeClassifier(criterion<span class="op">=</span>crit)</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> model.fit(x_train,y_train)</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>    yp_train<span class="op">=</span>model.predict(x_train)</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>    yp_test<span class="op">=</span>model.predict(x_test)</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>    a,b <span class="op">=</span> tunningResult(y_train, y_test, yp_train, yp_test,test_results, train_results)</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># print(y_pred.shape)</span></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>    test_results.append([crit]<span class="op">+</span>b)</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>    train_results.append([crit]<span class="op">+</span>a)</span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>test_results <span class="op">=</span> pd.DataFrame(test_results)</span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>train_results <span class="op">=</span> pd.DataFrame(train_results)</span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(test_results)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>         0         1         2         3         4
0     gini  0.387223  0.366834  0.349823  0.438596
1  entropy  0.434159  0.386935  0.445230  0.456140</code></pre>
</div>
</div>
<p>Considering the result from the accuracy and recall, I decided to choose a max_depth value of 4. In the plots we can see that when max_depth = 4, the accuracy of the prediction is stable and the recall for all three labels is high. For the hyperparameter min_samples_split, the optimized samples size for splitting is around 60 where the training and testing accuracy is parallel to each other and stabalized. Lastly, the criterion I will chose for the final model is entropy because it presents slightly higher accuracy.</p>
</section>
<section id="final-result-and-conclusion" class="level2">
<h2 class="anchored" data-anchor-id="final-result-and-conclusion">Final Result and Conclusion</h2>
<p>Finally, I used the set of optimized hyperparameters of max_depth, min_samples_split and criterion to fit a model on the label and the outputs are showing below. From the classification report, we can see that the accuracy of the testing set is around 47.2% which is only 14% higher than the accuracy of the baseline model. This result is unsatisfying for a three-label dataset. I think it will be better if I performed a Decision Tree Regression on the outcome variable without binning it to labels. Because the relationship between the x features and the outcome variable might be very linear which a classification cannot capture.</p>
<p>At this point, a decision tree classification is complete. A model that meant to predict new data points on their label is trained using the existing dataset. When we input a value or a set of values of the new features (X variable) into the model, the likelihood the the model outputting the true label is 47.2%. I have to admit that the accuracy here is not satisfying. But the process of how to apply Decision Tree classification on data is wholely performed and the overall accuracy has increased compared to the baseline model.</p>
<div class="cell" data-execution_count="15">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> tree.DecisionTreeClassifier(max_depth <span class="op">=</span> <span class="dv">4</span>, min_samples_split <span class="op">=</span> <span class="dv">60</span>, criterion<span class="op">=</span><span class="st">'entropy'</span>)</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> model.fit(x_train,y_train)</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>yp_train<span class="op">=</span>model.predict(x_train)</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>yp_test<span class="op">=</span>model.predict(x_test)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="16">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>confusion_plot(y_test,yp_test)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>----------------------Classification Report---------------------------
ACCURACY: 0.47196870925684486
RECALL (Y = Free): 0.40703517587939697
RECALL (Y = Coinsurance): 0.3992932862190813
RECALL (Y = Deductible): 0.5894736842105263
PRECISION (Y = Free): 0.47368421052631576
PRECISION (Y = Coinsurance): 0.39372822299651566
PRECISION (Y = Deductible): 0.5436893203883495
--------------------------Confusion Matrix-----------------------------
[[ 81  78  40]
 [ 69 113 101]
 [ 21  96 168]]</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="decision_tree_files/figure-html/cell-17-output-2.png" width="504" height="429"></p>
</div>
</div>
<div class="cell" data-execution_count="17">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> tree</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_tree(model):</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>    fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">20</span>,<span class="dv">15</span>))</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>    _ <span class="op">=</span> tree.plot_tree(model, </span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>                    filled<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>plot_tree(model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="decision_tree_files/figure-html/cell-18-output-1.png" width="1507" height="1128"></p>
</div>
</div>
<!-- -->

</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb23" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="an">jupyter:</span><span class="co"> python3</span></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Decision Tree on Record Data"</span></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "10/24/2022"</span></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a><span class="an">pdf-engine:</span><span class="co"> lualatex</span></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a><span class="co">    theme : Minty</span></span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a><span class="co">    toc: true</span></span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a><span class="co">    code-tools: true</span></span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a><span class="co">    code-fold: true</span></span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a><span class="co">    code-summary: "Code"</span></span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a><span class="co">    toc-title: Contents</span></span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a><span class="an">execute:</span></span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a><span class="co">    warning: false</span></span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a><span class="fu">### Methods</span></span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a>A tree has many analogies in real life, and it turns out that it has influenced a wide area of machine learning, covering both classification and regression. In decision analysis, a decision tree can be used to visually and explicitly represent decisions and decision making. As the name goes, it uses a tree-like model of decisions. Before jumping into a simple example, let's first take a look at some important terminologies in Decision Tree algorithm.   </span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a>|Terminology   |Meaning   |</span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a>|---|---|---|</span>
<span id="cb23-24"><a href="#cb23-24" aria-hidden="true" tabindex="-1"></a>| Root Node  | Represents entire population  |</span>
<span id="cb23-25"><a href="#cb23-25" aria-hidden="true" tabindex="-1"></a>| Spliting  | Process of dividing sample  |</span>
<span id="cb23-26"><a href="#cb23-26" aria-hidden="true" tabindex="-1"></a>| Decision Node  | Node splits into further sub (children) nodes  |</span>
<span id="cb23-27"><a href="#cb23-27" aria-hidden="true" tabindex="-1"></a>| Leaf Node  | Last stage of node (output label)  |</span>
<span id="cb23-28"><a href="#cb23-28" aria-hidden="true" tabindex="-1"></a>| Pruning  | The opposite process of splitting  |</span>
<span id="cb23-29"><a href="#cb23-29" aria-hidden="true" tabindex="-1"></a>Cheat-Sheet: DT terminologies: </span>
<span id="cb23-30"><a href="#cb23-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-31"><a href="#cb23-31" aria-hidden="true" tabindex="-1"></a>**How can an algorithm be represented as a tree**  </span>
<span id="cb23-32"><a href="#cb23-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-33"><a href="#cb23-33" aria-hidden="true" tabindex="-1"></a>For this let’s consider a very basic example that predicting whether a student will get an A in ANLY501. Below model uses 3 features/attributes/columns, namely desire, attendence and  time spent weekly(20hrs).</span>
<span id="cb23-34"><a href="#cb23-34" aria-hidden="true" tabindex="-1"></a>A decision tree is drawn upside down with its root at the top. In this image, the text in black represents a condition/internal **node**, based on which the tree splits into **branches/ edges**. The end of the branch that doesn’t split anymore is the decision/**leaf**, in this case, whether the student get an A or not is represented as text respectively.</span>
<span id="cb23-35"><a href="#cb23-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-38"><a href="#cb23-38" aria-hidden="true" tabindex="-1"></a><span class="in">```{mermaid}</span></span>
<span id="cb23-39"><a href="#cb23-39" aria-hidden="true" tabindex="-1"></a>flowchart TD</span>
<span id="cb23-40"><a href="#cb23-40" aria-hidden="true" tabindex="-1"></a>    A[Spend &gt;<span class="dv">2</span>0hrs/week] -- Yes --&gt; C[A!]</span>
<span id="cb23-41"><a href="#cb23-41" aria-hidden="true" tabindex="-1"></a>    A[Spend &gt;<span class="dv">2</span>0hrs/week] -- No --&gt; D[Desire of getting A?]</span>
<span id="cb23-42"><a href="#cb23-42" aria-hidden="true" tabindex="-1"></a>    D -- Yes --&gt; E[Attendence to lecture?]</span>
<span id="cb23-43"><a href="#cb23-43" aria-hidden="true" tabindex="-1"></a>    D -- No --&gt; F[No A!]</span>
<span id="cb23-44"><a href="#cb23-44" aria-hidden="true" tabindex="-1"></a>    E -- Yes --&gt; G[A!]</span>
<span id="cb23-45"><a href="#cb23-45" aria-hidden="true" tabindex="-1"></a>    E -- No --&gt; H[No A!]</span>
<span id="cb23-46"><a href="#cb23-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-47"><a href="#cb23-47" aria-hidden="true" tabindex="-1"></a><span class="in">```</span>  </span>
<span id="cb23-48"><a href="#cb23-48" aria-hidden="true" tabindex="-1"></a>**What is actually going on in the background?** </span>
<span id="cb23-49"><a href="#cb23-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-50"><a href="#cb23-50" aria-hidden="true" tabindex="-1"></a>A real dataset will have a lot more features and the above example will just be a branch in a much bigger tree, but we can’t ignore the simplicity of this algorithm. The feature importance is clear and relations can be viewed easily. This methodology is more commonly known as learning decision tree from data and above tree is called Classification tree as the target is to classify student as getting an A or not.</span>
<span id="cb23-51"><a href="#cb23-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-52"><a href="#cb23-52" aria-hidden="true" tabindex="-1"></a>Decision tree learning or growing a tree involves deciding on which features to choose and what conditions to use for splitting, along with knowing when to stop. As mentioned above, a decision tree is composed of nodes, and those nodes are chosen looking for the optimum split of the features. For that purpose, different criteria exist. In the decision tree Python implementation of the scikit-learn library, this is made by the parameter ‘criterion‘. This parameter is the function used to measure the quality of a split and it allows users to choose between **‘gini‘** or **‘entropy‘**.</span>
<span id="cb23-53"><a href="#cb23-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-54"><a href="#cb23-54" aria-hidden="true" tabindex="-1"></a>**-Gini Index**  </span>
<span id="cb23-55"><a href="#cb23-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-56"><a href="#cb23-56" aria-hidden="true" tabindex="-1"></a>The gini impurity is calculated using the following formula:  </span>
<span id="cb23-57"><a href="#cb23-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-58"><a href="#cb23-58" aria-hidden="true" tabindex="-1"></a>$$ GiniIndex = 1 - \sum_{j}p^2_{j}</span>
<span id="cb23-59"><a href="#cb23-59" aria-hidden="true" tabindex="-1"></a>$$  </span>
<span id="cb23-60"><a href="#cb23-60" aria-hidden="true" tabindex="-1"></a>Where $p_{j}$ is the probability of class $j$. </span>
<span id="cb23-61"><a href="#cb23-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-62"><a href="#cb23-62" aria-hidden="true" tabindex="-1"></a>The gini impurity measures the frequency at which any element of the dataset will be mislabelled when it is randomly labeled.</span>
<span id="cb23-63"><a href="#cb23-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-64"><a href="#cb23-64" aria-hidden="true" tabindex="-1"></a>**Entropy** </span>
<span id="cb23-65"><a href="#cb23-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-66"><a href="#cb23-66" aria-hidden="true" tabindex="-1"></a>The entropy is calculated using the following formula: </span>
<span id="cb23-67"><a href="#cb23-67" aria-hidden="true" tabindex="-1"></a>$$Entropy = -\sum_{j}p_{j}*log_{2}*p_{j}</span>
<span id="cb23-68"><a href="#cb23-68" aria-hidden="true" tabindex="-1"></a>$$  </span>
<span id="cb23-69"><a href="#cb23-69" aria-hidden="true" tabindex="-1"></a>Where, as before, $p_{j}$ is the probability of class $j$.  </span>
<span id="cb23-70"><a href="#cb23-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-71"><a href="#cb23-71" aria-hidden="true" tabindex="-1"></a>Entropy is a measure of information that indicates the disorder of the features with the target. Similar to the Gini Index, the optimum split is chosen by the feature with less entropy. It gets its maximum value when the probability of the two classes is the same and a node is pure when the entropy has its minimum value, which is 0.</span>
<span id="cb23-72"><a href="#cb23-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-73"><a href="#cb23-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-74"><a href="#cb23-74" aria-hidden="true" tabindex="-1"></a><span class="fu">### Class distribution</span></span>
<span id="cb23-75"><a href="#cb23-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-76"><a href="#cb23-76" aria-hidden="true" tabindex="-1"></a>Now let us come back to our reord dataset and implement the Decision Tree algorithm on it to classify the outcome variable -- Health Index. Since the outcome variable health index is a continuous variable which takes value from 0-100 and also since we want to do a classficication instead of a regression here, I binned the column to multiple classes and assign them with a label. The following table shows the range, count and ratio of each label：</span>
<span id="cb23-77"><a href="#cb23-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-78"><a href="#cb23-78" aria-hidden="true" tabindex="-1"></a>|Health Index   |Label   | count | Probability |</span>
<span id="cb23-79"><a href="#cb23-79" aria-hidden="true" tabindex="-1"></a>|---|---|---|---|</span>
<span id="cb23-80"><a href="#cb23-80" aria-hidden="true" tabindex="-1"></a>| Below 60  | 0  | 974 | 0.381 |</span>
<span id="cb23-81"><a href="#cb23-81" aria-hidden="true" tabindex="-1"></a>| 60-75  | 1  | 903 | 0.354 |</span>
<span id="cb23-82"><a href="#cb23-82" aria-hidden="true" tabindex="-1"></a>| &gt;75  | 2  | 677 | 0.265 |</span>
<span id="cb23-83"><a href="#cb23-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-84"><a href="#cb23-84" aria-hidden="true" tabindex="-1"></a>According to both the printed table and barplot, we can say that the distribution of three labels is pretty balanced.</span>
<span id="cb23-85"><a href="#cb23-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-86"><a href="#cb23-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-87"><a href="#cb23-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-88"><a href="#cb23-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-91"><a href="#cb23-91" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb23-92"><a href="#cb23-92" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb23-93"><a href="#cb23-93" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb23-94"><a href="#cb23-94" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb23-95"><a href="#cb23-95" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb23-96"><a href="#cb23-96" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix, ConfusionMatrixDisplay</span>
<span id="cb23-97"><a href="#cb23-97" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> tree</span>
<span id="cb23-98"><a href="#cb23-98" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span>
<span id="cb23-99"><a href="#cb23-99" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> precision_score</span>
<span id="cb23-100"><a href="#cb23-100" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> recall_score</span>
<span id="cb23-101"><a href="#cb23-101" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb23-102"><a href="#cb23-102" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_selection <span class="im">import</span> VarianceThreshold</span>
<span id="cb23-103"><a href="#cb23-103" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb23-104"><a href="#cb23-104" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-105"><a href="#cb23-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-108"><a href="#cb23-108" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb23-109"><a href="#cb23-109" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">'/Users/liumingqian/anly-501-project-liumingqian0511/data/01-modified-data/rand.csv'</span>)</span>
<span id="cb23-110"><a href="#cb23-110" aria-hidden="true" tabindex="-1"></a>conditions <span class="op">=</span> [</span>
<span id="cb23-111"><a href="#cb23-111" aria-hidden="true" tabindex="-1"></a>    (df[<span class="st">'ghindxx'</span>] <span class="op">&lt;</span> <span class="dv">60</span>),</span>
<span id="cb23-112"><a href="#cb23-112" aria-hidden="true" tabindex="-1"></a>    (df[<span class="st">'ghindxx'</span>] <span class="op">&gt;=</span> <span class="dv">60</span>) <span class="op">&amp;</span> (df[<span class="st">'ghindxx'</span>]<span class="op">&lt;</span><span class="dv">75</span>),</span>
<span id="cb23-113"><a href="#cb23-113" aria-hidden="true" tabindex="-1"></a>    (df[<span class="st">'ghindxx'</span>] <span class="op">&gt;=</span> <span class="dv">75</span>),</span>
<span id="cb23-114"><a href="#cb23-114" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb23-115"><a href="#cb23-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-116"><a href="#cb23-116" aria-hidden="true" tabindex="-1"></a>values <span class="op">=</span> [<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>]</span>
<span id="cb23-117"><a href="#cb23-117" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'health_index'</span>] <span class="op">=</span> np.select(conditions, values)</span>
<span id="cb23-118"><a href="#cb23-118" aria-hidden="true" tabindex="-1"></a>distribution <span class="op">=</span> df[<span class="st">'health_index'</span>].value_counts().reset_index().rename(columns<span class="op">=</span>{<span class="st">'index'</span>: <span class="st">'health_index'</span>,<span class="st">'health_index'</span>:<span class="st">'count'</span>})</span>
<span id="cb23-119"><a href="#cb23-119" aria-hidden="true" tabindex="-1"></a>distribution[<span class="st">'proportion'</span>] <span class="op">=</span> distribution[<span class="st">'count'</span>]<span class="op">/</span><span class="bu">len</span>(df)</span>
<span id="cb23-120"><a href="#cb23-120" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'-----------------------------------'</span>)</span>
<span id="cb23-121"><a href="#cb23-121" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(distribution)</span>
<span id="cb23-122"><a href="#cb23-122" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'-----------------------------------'</span>)</span>
<span id="cb23-123"><a href="#cb23-123" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> sns.barplot(x <span class="op">=</span> <span class="st">'health_index'</span>, y <span class="op">=</span> <span class="st">'count'</span>, data <span class="op">=</span> distribution)</span>
<span id="cb23-124"><a href="#cb23-124" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Barplot of Health Index Range'</span>)</span>
<span id="cb23-125"><a href="#cb23-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-126"><a href="#cb23-126" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb23-127"><a href="#cb23-127" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-128"><a href="#cb23-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-129"><a href="#cb23-129" aria-hidden="true" tabindex="-1"></a><span class="fu">### Baseline Model</span></span>
<span id="cb23-130"><a href="#cb23-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-133"><a href="#cb23-133" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb23-134"><a href="#cb23-134" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> random_classifier(y_data):</span>
<span id="cb23-135"><a href="#cb23-135" aria-hidden="true" tabindex="-1"></a>    random.seed(<span class="dv">909391</span>)</span>
<span id="cb23-136"><a href="#cb23-136" aria-hidden="true" tabindex="-1"></a>    ypred<span class="op">=</span>[]</span>
<span id="cb23-137"><a href="#cb23-137" aria-hidden="true" tabindex="-1"></a>    max_label<span class="op">=</span>np.<span class="bu">max</span>(y_data)</span>
<span id="cb23-138"><a href="#cb23-138" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>,<span class="bu">len</span>(y_data)):</span>
<span id="cb23-139"><a href="#cb23-139" aria-hidden="true" tabindex="-1"></a>        ypred.append(np.random.choice(np.arange(max_label<span class="op">+</span><span class="dv">1</span>), p <span class="op">=</span> distribution[<span class="st">'proportion'</span>]))</span>
<span id="cb23-140"><a href="#cb23-140" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> ypred</span>
<span id="cb23-141"><a href="#cb23-141" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-142"><a href="#cb23-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-143"><a href="#cb23-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-144"><a href="#cb23-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-147"><a href="#cb23-147" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb23-148"><a href="#cb23-148" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> confusion_plot(y_data,y_pred):</span>
<span id="cb23-149"><a href="#cb23-149" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'----------------------Classification Report---------------------------'</span>)</span>
<span id="cb23-150"><a href="#cb23-150" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'ACCURACY:'</span>, accuracy_score(y_data, y_pred))</span>
<span id="cb23-151"><a href="#cb23-151" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'RECALL (Y = Free):'</span>, recall_score(y_data, y_pred,labels<span class="op">=</span>[<span class="dv">0</span>], average<span class="op">=</span><span class="st">'weighted'</span>))</span>
<span id="cb23-152"><a href="#cb23-152" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'RECALL (Y = Coinsurance):'</span>, recall_score(y_data, y_pred,labels<span class="op">=</span>[<span class="dv">1</span>], average<span class="op">=</span><span class="st">'weighted'</span>))</span>
<span id="cb23-153"><a href="#cb23-153" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'RECALL (Y = Deductible):'</span>, recall_score(y_data, y_pred,labels<span class="op">=</span>[<span class="dv">2</span>], average<span class="op">=</span><span class="st">'weighted'</span>))</span>
<span id="cb23-154"><a href="#cb23-154" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'PRECISION (Y = Free):'</span>, precision_score(y_data,y_pred,labels<span class="op">=</span>[<span class="dv">0</span>],average<span class="op">=</span><span class="st">'weighted'</span>))</span>
<span id="cb23-155"><a href="#cb23-155" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'PRECISION (Y = Coinsurance):'</span>, precision_score(y_data,y_pred,labels<span class="op">=</span>[<span class="dv">1</span>], average<span class="op">=</span><span class="st">'weighted'</span>))</span>
<span id="cb23-156"><a href="#cb23-156" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'PRECISION (Y = Deductible):'</span>, precision_score(y_data,y_pred,labels<span class="op">=</span>[<span class="dv">2</span>], average<span class="op">=</span><span class="st">'weighted'</span>))</span>
<span id="cb23-157"><a href="#cb23-157" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'--------------------------Confusion Matrix-----------------------------'</span>)</span>
<span id="cb23-158"><a href="#cb23-158" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(confusion_matrix(y_data,y_pred))</span>
<span id="cb23-159"><a href="#cb23-159" aria-hidden="true" tabindex="-1"></a>    ConfusionMatrixDisplay.from_predictions(y_data, y_pred)</span>
<span id="cb23-160"><a href="#cb23-160" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb23-161"><a href="#cb23-161" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-162"><a href="#cb23-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-165"><a href="#cb23-165" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb23-166"><a href="#cb23-166" aria-hidden="true" tabindex="-1"></a>true_label <span class="op">=</span> df[<span class="st">'health_index'</span>]</span>
<span id="cb23-167"><a href="#cb23-167" aria-hidden="true" tabindex="-1"></a>predicted_label <span class="op">=</span> random_classifier(true_label)</span>
<span id="cb23-168"><a href="#cb23-168" aria-hidden="true" tabindex="-1"></a>confusion_plot(true_label,predicted_label)</span>
<span id="cb23-169"><a href="#cb23-169" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-170"><a href="#cb23-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-171"><a href="#cb23-171" aria-hidden="true" tabindex="-1"></a><span class="fu">### Feature Selection</span></span>
<span id="cb23-172"><a href="#cb23-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-175"><a href="#cb23-175" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb23-176"><a href="#cb23-176" aria-hidden="true" tabindex="-1"></a>conditions <span class="op">=</span> [</span>
<span id="cb23-177"><a href="#cb23-177" aria-hidden="true" tabindex="-1"></a>    (df[<span class="st">'plan_type'</span>] <span class="op">==</span> <span class="st">'Free'</span>),</span>
<span id="cb23-178"><a href="#cb23-178" aria-hidden="true" tabindex="-1"></a>    (df[<span class="st">'plan_type'</span>] <span class="op">==</span> <span class="st">'Coinsurance'</span>),</span>
<span id="cb23-179"><a href="#cb23-179" aria-hidden="true" tabindex="-1"></a>    (df[<span class="st">'plan_type'</span>] <span class="op">==</span> <span class="st">'Deductible'</span>),</span>
<span id="cb23-180"><a href="#cb23-180" aria-hidden="true" tabindex="-1"></a>    (df[<span class="st">'plan_type'</span>] <span class="op">==</span> <span class="st">'Catastrophic'</span>)</span>
<span id="cb23-181"><a href="#cb23-181" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb23-182"><a href="#cb23-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-183"><a href="#cb23-183" aria-hidden="true" tabindex="-1"></a>values <span class="op">=</span> [<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>]</span>
<span id="cb23-184"><a href="#cb23-184" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'plan'</span>] <span class="op">=</span> np.select(conditions, values)</span>
<span id="cb23-185"><a href="#cb23-185" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-186"><a href="#cb23-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-189"><a href="#cb23-189" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb23-190"><a href="#cb23-190" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> df[<span class="st">'health_index'</span>]</span>
<span id="cb23-191"><a href="#cb23-191" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df.drop(columns<span class="op">=</span>[<span class="st">'person'</span>,<span class="st">'plan_type'</span>,<span class="st">'catastrophic'</span>,<span class="st">'free'</span>,<span class="st">'health_index'</span>,<span class="st">'ghindxx'</span>,<span class="st">'face_to_face_visit'</span>,<span class="st">'outpatient_expenses'</span>,<span class="st">'totadm'</span>,<span class="st">'total_expenses'</span>,<span class="st">'inpdol_inf'</span>])</span>
<span id="cb23-192"><a href="#cb23-192" aria-hidden="true" tabindex="-1"></a>ux <span class="op">=</span> np.mean(X,axis <span class="op">=</span> <span class="dv">0</span>) <span class="co"># NORMALIZE X</span></span>
<span id="cb23-193"><a href="#cb23-193" aria-hidden="true" tabindex="-1"></a>sx <span class="op">=</span> np.std(X,axis <span class="op">=</span> <span class="dv">0</span>)</span>
<span id="cb23-194"><a href="#cb23-194" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb23-195"><a href="#cb23-195" aria-hidden="true" tabindex="-1"></a>    X.iloc[:,i] <span class="op">=</span> (X.iloc[:,i] <span class="op">-</span> ux[i])<span class="op">/</span>sx[i]</span>
<span id="cb23-196"><a href="#cb23-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-197"><a href="#cb23-197" aria-hidden="true" tabindex="-1"></a><span class="co">#X['plan'] = df['plan']</span></span>
<span id="cb23-198"><a href="#cb23-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-199"><a href="#cb23-199" aria-hidden="true" tabindex="-1"></a>x_train, x_test, y_train, y_test <span class="op">=</span> train_test_split(X, Y, test_size<span class="op">=</span><span class="fl">0.3</span>, random_state<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb23-200"><a href="#cb23-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-201"><a href="#cb23-201" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"TRAINING SHAPES:"</span>,x_train.shape,y_train.shape)</span>
<span id="cb23-202"><a href="#cb23-202" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"TEST SHAPES:"</span>,x_test.shape,y_test.shape)</span>
<span id="cb23-203"><a href="#cb23-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-204"><a href="#cb23-204" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-205"><a href="#cb23-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-206"><a href="#cb23-206" aria-hidden="true" tabindex="-1"></a><span class="fu">## Hyperparameter Tuning</span></span>
<span id="cb23-207"><a href="#cb23-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-208"><a href="#cb23-208" aria-hidden="true" tabindex="-1"></a>Hyperparameter tuning consists of finding a set of optimal hyperparameter values for a learning algorithm while applying this optimized algorithm to any data set. That combination of hyperparameters maximizes the model's performance, minimizing a predefined loss function to produce better results with fewer errors. </span>
<span id="cb23-209"><a href="#cb23-209" aria-hidden="true" tabindex="-1"></a>In this section, I am going to tune max_depth, min_sample_split and criterion parameter.</span>
<span id="cb23-210"><a href="#cb23-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-211"><a href="#cb23-211" aria-hidden="true" tabindex="-1"></a>**Max_depth Tuning:** </span>
<span id="cb23-212"><a href="#cb23-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-213"><a href="#cb23-213" aria-hidden="true" tabindex="-1"></a>The first hyperparameter to tune in the Decision Tree is max_depth. It indicates how deep the decision can be. The deeper the tree, the more splits it has and meanwhile captures more information about the data. However, in general a decision tree overfits for large depth values. When the tree perfectly predicts all the training data, it failes to generalize the findings on new data. </span>
<span id="cb23-214"><a href="#cb23-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-215"><a href="#cb23-215" aria-hidden="true" tabindex="-1"></a>**Min_sample_split Tuning** </span>
<span id="cb23-216"><a href="#cb23-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-217"><a href="#cb23-217" aria-hidden="true" tabindex="-1"></a>The hyperparameter min_sample_split is used to set the minimum number of samples required to split an internal node. This can vary between two extremes, i.e., considering only one sample at each node vs. considering all of the samples at each node - for a given attribute.</span>
<span id="cb23-218"><a href="#cb23-218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-219"><a href="#cb23-219" aria-hidden="true" tabindex="-1"></a>**Note**: max_depth and min_samples_split are also both related to the computational cost involved with growing the tree. Large values for these parameters can create complex, dense, and long trees. For large datasets, it may become extremely time-consuming to use default values.</span>
<span id="cb23-220"><a href="#cb23-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-221"><a href="#cb23-221" aria-hidden="true" tabindex="-1"></a>**Criterion**</span>
<span id="cb23-222"><a href="#cb23-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-223"><a href="#cb23-223" aria-hidden="true" tabindex="-1"></a>Supported Criterion are 'Gini' and 'Entropy'.</span>
<span id="cb23-224"><a href="#cb23-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-227"><a href="#cb23-227" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb23-228"><a href="#cb23-228" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tunningResult(y_train, y_test, yp_train, yp_test,test_result, train_result):</span>
<span id="cb23-229"><a href="#cb23-229" aria-hidden="true" tabindex="-1"></a>    accuracy_train <span class="op">=</span> accuracy_score(y_train, yp_train)</span>
<span id="cb23-230"><a href="#cb23-230" aria-hidden="true" tabindex="-1"></a>    recall_0_train <span class="op">=</span> recall_score(y_train, yp_train,labels<span class="op">=</span>[<span class="dv">0</span>], average<span class="op">=</span><span class="st">'weighted'</span>)</span>
<span id="cb23-231"><a href="#cb23-231" aria-hidden="true" tabindex="-1"></a>    recall_1_train <span class="op">=</span> recall_score(y_train, yp_train,labels<span class="op">=</span>[<span class="dv">1</span>], average<span class="op">=</span><span class="st">'weighted'</span>)</span>
<span id="cb23-232"><a href="#cb23-232" aria-hidden="true" tabindex="-1"></a>    recall_2_train <span class="op">=</span> recall_score(y_train, yp_train,labels<span class="op">=</span>[<span class="dv">2</span>], average<span class="op">=</span><span class="st">'weighted'</span>)</span>
<span id="cb23-233"><a href="#cb23-233" aria-hidden="true" tabindex="-1"></a>    <span class="co">#recall_3_train = recall_score(y_train, yp_train,labels=[3], average='weighted')</span></span>
<span id="cb23-234"><a href="#cb23-234" aria-hidden="true" tabindex="-1"></a>    accuracy_test <span class="op">=</span> accuracy_score(y_test, yp_test)</span>
<span id="cb23-235"><a href="#cb23-235" aria-hidden="true" tabindex="-1"></a>    recall_0_test <span class="op">=</span> recall_score(y_test, yp_test,labels<span class="op">=</span>[<span class="dv">0</span>], average<span class="op">=</span><span class="st">'weighted'</span>)</span>
<span id="cb23-236"><a href="#cb23-236" aria-hidden="true" tabindex="-1"></a>    recall_1_test <span class="op">=</span> recall_score(y_test, yp_test,labels<span class="op">=</span>[<span class="dv">1</span>], average<span class="op">=</span><span class="st">'weighted'</span>)</span>
<span id="cb23-237"><a href="#cb23-237" aria-hidden="true" tabindex="-1"></a>    recall_2_test <span class="op">=</span> recall_score(y_test, yp_test,labels<span class="op">=</span>[<span class="dv">2</span>], average<span class="op">=</span><span class="st">'weighted'</span>)</span>
<span id="cb23-238"><a href="#cb23-238" aria-hidden="true" tabindex="-1"></a>    <span class="co">#recall_3_test = recall_score(y_test, yp_test,labels=[3], average='weighted')</span></span>
<span id="cb23-239"><a href="#cb23-239" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> [accuracy_train, recall_0_train,recall_1_train,recall_2_train],[accuracy_test,recall_0_test,recall_1_test,recall_2_test]</span>
<span id="cb23-240"><a href="#cb23-240" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-241"><a href="#cb23-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-244"><a href="#cb23-244" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb23-245"><a href="#cb23-245" aria-hidden="true" tabindex="-1"></a>test_results<span class="op">=</span>[]</span>
<span id="cb23-246"><a href="#cb23-246" aria-hidden="true" tabindex="-1"></a>train_results<span class="op">=</span>[]</span>
<span id="cb23-247"><a href="#cb23-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-248"><a href="#cb23-248" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> depth <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,<span class="dv">16</span>):</span>
<span id="cb23-249"><a href="#cb23-249" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> tree.DecisionTreeClassifier(max_depth<span class="op">=</span>depth)</span>
<span id="cb23-250"><a href="#cb23-250" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> model.fit(x_train,y_train)</span>
<span id="cb23-251"><a href="#cb23-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-252"><a href="#cb23-252" aria-hidden="true" tabindex="-1"></a>    yp_train<span class="op">=</span>model.predict(x_train)</span>
<span id="cb23-253"><a href="#cb23-253" aria-hidden="true" tabindex="-1"></a>    yp_test<span class="op">=</span>model.predict(x_test)</span>
<span id="cb23-254"><a href="#cb23-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-255"><a href="#cb23-255" aria-hidden="true" tabindex="-1"></a>    a,b <span class="op">=</span> tunningResult(y_train, y_test, yp_train, yp_test,test_results, train_results)</span>
<span id="cb23-256"><a href="#cb23-256" aria-hidden="true" tabindex="-1"></a>    <span class="co"># print(y_pred.shape)</span></span>
<span id="cb23-257"><a href="#cb23-257" aria-hidden="true" tabindex="-1"></a>    test_results.append([depth]<span class="op">+</span>b)</span>
<span id="cb23-258"><a href="#cb23-258" aria-hidden="true" tabindex="-1"></a>    train_results.append([depth]<span class="op">+</span>a)</span>
<span id="cb23-259"><a href="#cb23-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-260"><a href="#cb23-260" aria-hidden="true" tabindex="-1"></a>test_results <span class="op">=</span> pd.DataFrame(test_results)</span>
<span id="cb23-261"><a href="#cb23-261" aria-hidden="true" tabindex="-1"></a>train_results <span class="op">=</span> pd.DataFrame(train_results)</span>
<span id="cb23-262"><a href="#cb23-262" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-263"><a href="#cb23-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-264"><a href="#cb23-264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-265"><a href="#cb23-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-268"><a href="#cb23-268" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb23-269"><a href="#cb23-269" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> resultPlot(train_results,test_results,param):</span>
<span id="cb23-270"><a href="#cb23-270" aria-hidden="true" tabindex="-1"></a>    fig,((ax1,ax2),(ax3,ax4)) <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb23-271"><a href="#cb23-271" aria-hidden="true" tabindex="-1"></a>    fig.set_size_inches(<span class="dv">20</span>,<span class="dv">10</span>)</span>
<span id="cb23-272"><a href="#cb23-272" aria-hidden="true" tabindex="-1"></a>    ax1.plot(train_results[<span class="dv">0</span>],train_results[<span class="dv">1</span>],<span class="st">'-o'</span>)</span>
<span id="cb23-273"><a href="#cb23-273" aria-hidden="true" tabindex="-1"></a>    ax1.plot(test_results[<span class="dv">0</span>],test_results[<span class="dv">1</span>],<span class="st">'-o'</span>,color <span class="op">=</span> <span class="st">'darkred'</span>)</span>
<span id="cb23-274"><a href="#cb23-274" aria-hidden="true" tabindex="-1"></a>    ax1.set_xlabel(<span class="st">"Number of layers in decision tree"</span> <span class="op">+</span> param)</span>
<span id="cb23-275"><a href="#cb23-275" aria-hidden="true" tabindex="-1"></a>    ax1.set_ylabel(<span class="st">"ACCURACY: Training and Test"</span>)</span>
<span id="cb23-276"><a href="#cb23-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-277"><a href="#cb23-277" aria-hidden="true" tabindex="-1"></a>    ax2.plot(train_results[<span class="dv">0</span>],train_results[<span class="dv">2</span>],<span class="st">'-o'</span>)</span>
<span id="cb23-278"><a href="#cb23-278" aria-hidden="true" tabindex="-1"></a>    ax2.plot(test_results[<span class="dv">0</span>],test_results[<span class="dv">2</span>],<span class="st">'-o'</span>,color <span class="op">=</span> <span class="st">'darkred'</span>)</span>
<span id="cb23-279"><a href="#cb23-279" aria-hidden="true" tabindex="-1"></a>    ax2.set_xlabel(<span class="st">"Number of layers in decision tree"</span> <span class="op">+</span> param)</span>
<span id="cb23-280"><a href="#cb23-280" aria-hidden="true" tabindex="-1"></a>    ax2.set_ylabel(<span class="st">"RECALL(Y=Free): Training and Test"</span>)</span>
<span id="cb23-281"><a href="#cb23-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-282"><a href="#cb23-282" aria-hidden="true" tabindex="-1"></a>    ax3.plot(train_results[<span class="dv">0</span>],train_results[<span class="dv">3</span>],<span class="st">'-o'</span>)</span>
<span id="cb23-283"><a href="#cb23-283" aria-hidden="true" tabindex="-1"></a>    ax3.plot(test_results[<span class="dv">0</span>],test_results[<span class="dv">3</span>],<span class="st">'-o'</span>,color <span class="op">=</span> <span class="st">'darkred'</span>)</span>
<span id="cb23-284"><a href="#cb23-284" aria-hidden="true" tabindex="-1"></a>    ax3.set_xlabel(<span class="st">"Number of layers in decision tree"</span> <span class="op">+</span> param)</span>
<span id="cb23-285"><a href="#cb23-285" aria-hidden="true" tabindex="-1"></a>    ax3.set_ylabel(<span class="st">"RECALL(Y=Coinsurance): Training and Test"</span>)</span>
<span id="cb23-286"><a href="#cb23-286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-287"><a href="#cb23-287" aria-hidden="true" tabindex="-1"></a>    ax4.plot(train_results[<span class="dv">0</span>],train_results[<span class="dv">4</span>],<span class="st">'-o'</span>)</span>
<span id="cb23-288"><a href="#cb23-288" aria-hidden="true" tabindex="-1"></a>    ax4.plot(test_results[<span class="dv">0</span>],test_results[<span class="dv">4</span>],<span class="st">'-o'</span>,color <span class="op">=</span> <span class="st">'darkred'</span>)</span>
<span id="cb23-289"><a href="#cb23-289" aria-hidden="true" tabindex="-1"></a>    ax4.set_xlabel(<span class="st">"Number of layers in decision tree"</span> <span class="op">+</span> param)</span>
<span id="cb23-290"><a href="#cb23-290" aria-hidden="true" tabindex="-1"></a>    ax4.set_ylabel(<span class="st">"RECALL(Y=Deductible): Training and Test"</span>)</span>
<span id="cb23-291"><a href="#cb23-291" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb23-292"><a href="#cb23-292" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-293"><a href="#cb23-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-296"><a href="#cb23-296" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb23-297"><a href="#cb23-297" aria-hidden="true" tabindex="-1"></a>resultPlot(train_results,test_results, <span class="st">' max_depth'</span>)</span>
<span id="cb23-298"><a href="#cb23-298" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-299"><a href="#cb23-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-300"><a href="#cb23-300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-301"><a href="#cb23-301" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Min_sample_split Tuning</span>
<span id="cb23-302"><a href="#cb23-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-305"><a href="#cb23-305" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb23-306"><a href="#cb23-306" aria-hidden="true" tabindex="-1"></a>test_results<span class="op">=</span>[]</span>
<span id="cb23-307"><a href="#cb23-307" aria-hidden="true" tabindex="-1"></a>train_results<span class="op">=</span>[]</span>
<span id="cb23-308"><a href="#cb23-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-309"><a href="#cb23-309" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> num_sample <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>,<span class="dv">100</span>):</span>
<span id="cb23-310"><a href="#cb23-310" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> tree.DecisionTreeClassifier(min_samples_split<span class="op">=</span>num_sample )</span>
<span id="cb23-311"><a href="#cb23-311" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> model.fit(x_train,y_train)</span>
<span id="cb23-312"><a href="#cb23-312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-313"><a href="#cb23-313" aria-hidden="true" tabindex="-1"></a>    yp_train<span class="op">=</span>model.predict(x_train)</span>
<span id="cb23-314"><a href="#cb23-314" aria-hidden="true" tabindex="-1"></a>    yp_test<span class="op">=</span>model.predict(x_test)</span>
<span id="cb23-315"><a href="#cb23-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-316"><a href="#cb23-316" aria-hidden="true" tabindex="-1"></a>    a,b <span class="op">=</span> tunningResult(y_train, y_test, yp_train, yp_test,test_results, train_results)</span>
<span id="cb23-317"><a href="#cb23-317" aria-hidden="true" tabindex="-1"></a>    <span class="co"># print(y_pred.shape)</span></span>
<span id="cb23-318"><a href="#cb23-318" aria-hidden="true" tabindex="-1"></a>    test_results.append([num_sample]<span class="op">+</span>b)</span>
<span id="cb23-319"><a href="#cb23-319" aria-hidden="true" tabindex="-1"></a>    train_results.append([num_sample]<span class="op">+</span>a)</span>
<span id="cb23-320"><a href="#cb23-320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-321"><a href="#cb23-321" aria-hidden="true" tabindex="-1"></a>test_results <span class="op">=</span> pd.DataFrame(test_results)</span>
<span id="cb23-322"><a href="#cb23-322" aria-hidden="true" tabindex="-1"></a>train_results <span class="op">=</span> pd.DataFrame(train_results)</span>
<span id="cb23-323"><a href="#cb23-323" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-324"><a href="#cb23-324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-325"><a href="#cb23-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-326"><a href="#cb23-326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-329"><a href="#cb23-329" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb23-330"><a href="#cb23-330" aria-hidden="true" tabindex="-1"></a>resultPlot(train_results,test_results, <span class="st">' min_sample_split'</span>)</span>
<span id="cb23-331"><a href="#cb23-331" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-332"><a href="#cb23-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-335"><a href="#cb23-335" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb23-336"><a href="#cb23-336" aria-hidden="true" tabindex="-1"></a>test_results<span class="op">=</span>[]</span>
<span id="cb23-337"><a href="#cb23-337" aria-hidden="true" tabindex="-1"></a>train_results<span class="op">=</span>[]</span>
<span id="cb23-338"><a href="#cb23-338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-339"><a href="#cb23-339" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> crit <span class="kw">in</span> [<span class="st">'gini'</span>,<span class="st">'entropy'</span>]:</span>
<span id="cb23-340"><a href="#cb23-340" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> tree.DecisionTreeClassifier(criterion<span class="op">=</span>crit)</span>
<span id="cb23-341"><a href="#cb23-341" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> model.fit(x_train,y_train)</span>
<span id="cb23-342"><a href="#cb23-342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-343"><a href="#cb23-343" aria-hidden="true" tabindex="-1"></a>    yp_train<span class="op">=</span>model.predict(x_train)</span>
<span id="cb23-344"><a href="#cb23-344" aria-hidden="true" tabindex="-1"></a>    yp_test<span class="op">=</span>model.predict(x_test)</span>
<span id="cb23-345"><a href="#cb23-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-346"><a href="#cb23-346" aria-hidden="true" tabindex="-1"></a>    a,b <span class="op">=</span> tunningResult(y_train, y_test, yp_train, yp_test,test_results, train_results)</span>
<span id="cb23-347"><a href="#cb23-347" aria-hidden="true" tabindex="-1"></a>    <span class="co"># print(y_pred.shape)</span></span>
<span id="cb23-348"><a href="#cb23-348" aria-hidden="true" tabindex="-1"></a>    test_results.append([crit]<span class="op">+</span>b)</span>
<span id="cb23-349"><a href="#cb23-349" aria-hidden="true" tabindex="-1"></a>    train_results.append([crit]<span class="op">+</span>a)</span>
<span id="cb23-350"><a href="#cb23-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-351"><a href="#cb23-351" aria-hidden="true" tabindex="-1"></a>test_results <span class="op">=</span> pd.DataFrame(test_results)</span>
<span id="cb23-352"><a href="#cb23-352" aria-hidden="true" tabindex="-1"></a>train_results <span class="op">=</span> pd.DataFrame(train_results)</span>
<span id="cb23-353"><a href="#cb23-353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-354"><a href="#cb23-354" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(test_results)</span>
<span id="cb23-355"><a href="#cb23-355" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-356"><a href="#cb23-356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-357"><a href="#cb23-357" aria-hidden="true" tabindex="-1"></a>Considering the result from the accuracy and recall, I decided to choose a max_depth value of 4. In the plots we can see that when max_depth = 4, the accuracy of the prediction is stable and the recall for all three labels is high. For the hyperparameter min_samples_split, the optimized samples size for splitting is around 60 where the training and testing accuracy is parallel to each other and stabalized. Lastly, the criterion I will chose for the final model is entropy because it presents slightly higher accuracy.</span>
<span id="cb23-358"><a href="#cb23-358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-359"><a href="#cb23-359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-360"><a href="#cb23-360" aria-hidden="true" tabindex="-1"></a><span class="fu">## Final Result and Conclusion</span></span>
<span id="cb23-361"><a href="#cb23-361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-362"><a href="#cb23-362" aria-hidden="true" tabindex="-1"></a>Finally, I used the set of optimized hyperparameters of max_depth, min_samples_split and criterion to fit a model on the label and the outputs are showing below. From the classification report, we can see that the accuracy of the testing set is around 47.2% which is only 14% higher than the accuracy of the baseline model. This result is unsatisfying for a three-label dataset. I think it will be better if I performed a Decision Tree Regression on the outcome variable without binning it to labels. Because the relationship between the x features and the outcome variable might be very linear which a classification cannot capture. </span>
<span id="cb23-363"><a href="#cb23-363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-364"><a href="#cb23-364" aria-hidden="true" tabindex="-1"></a>At this point, a decision tree classification is complete. A model that meant to predict new data points on their label is trained using the existing dataset. When we input a value or a set of values of the new features (X variable) into the model, the likelihood the the model outputting the true label is 47.2%. I have to admit that the accuracy here is not satisfying. But the process of how to apply Decision Tree classification on data is wholely performed and the overall accuracy has increased compared to the baseline model. </span>
<span id="cb23-365"><a href="#cb23-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-368"><a href="#cb23-368" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb23-369"><a href="#cb23-369" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> tree.DecisionTreeClassifier(max_depth <span class="op">=</span> <span class="dv">4</span>, min_samples_split <span class="op">=</span> <span class="dv">60</span>, criterion<span class="op">=</span><span class="st">'entropy'</span>)</span>
<span id="cb23-370"><a href="#cb23-370" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> model.fit(x_train,y_train)</span>
<span id="cb23-371"><a href="#cb23-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-372"><a href="#cb23-372" aria-hidden="true" tabindex="-1"></a>yp_train<span class="op">=</span>model.predict(x_train)</span>
<span id="cb23-373"><a href="#cb23-373" aria-hidden="true" tabindex="-1"></a>yp_test<span class="op">=</span>model.predict(x_test)</span>
<span id="cb23-374"><a href="#cb23-374" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-375"><a href="#cb23-375" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-378"><a href="#cb23-378" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb23-379"><a href="#cb23-379" aria-hidden="true" tabindex="-1"></a>confusion_plot(y_test,yp_test)</span>
<span id="cb23-380"><a href="#cb23-380" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb23-381"><a href="#cb23-381" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-384"><a href="#cb23-384" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb23-385"><a href="#cb23-385" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> tree</span>
<span id="cb23-386"><a href="#cb23-386" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_tree(model):</span>
<span id="cb23-387"><a href="#cb23-387" aria-hidden="true" tabindex="-1"></a>    fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">20</span>,<span class="dv">15</span>))</span>
<span id="cb23-388"><a href="#cb23-388" aria-hidden="true" tabindex="-1"></a>    _ <span class="op">=</span> tree.plot_tree(model, </span>
<span id="cb23-389"><a href="#cb23-389" aria-hidden="true" tabindex="-1"></a>                    filled<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb23-390"><a href="#cb23-390" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb23-391"><a href="#cb23-391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-392"><a href="#cb23-392" aria-hidden="true" tabindex="-1"></a>plot_tree(model)</span>
<span id="cb23-393"><a href="#cb23-393" aria-hidden="true" tabindex="-1"></a><span class="in">```</span> </span>
<span id="cb23-394"><a href="#cb23-394" aria-hidden="true" tabindex="-1"></a></span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



</body></html>